---
title: "Inference for Linear Regression"
author: "Jeff Li"
date: "`r Sys.Date()`"
output: 
  html_document: 
    toc: true
    toc_depth: 3
    toc_float: true
---

“Give me six hours to chop down a tree and I will spend the first four sharpening the ax.”  ~ Abraham Lincoln 


**Course Description**

"Previously, you learned the fundamentals of both statistical inference and linear models; now, the next step is to put them together. This course gives you a chance to think about how different samples can produce different linear models, where your goal is to understand the underlying population model. From the estimated linear model, you will learn how to create interval estimates for the effect size as well as how to determine if the effect is significant. Prediction intervals for the response variable will be contrasted with estimates of the average response. Throughout the course, you'll gain more practice with the dplyr and ggplot2 packages, and you will learn about the broom package for tidying models; all three packages are invaluable in data science." 



Ref: Hardin, Jo. (2019) "Inference for Linear Regression". https://www.datacamp.com/courses


Note: Some course materials and data have been digested and adapted for my teaching. 



# (I) Load Required Libraries

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# (b) Load libraries
library(tidyverse)
library(NHANES)
library(infer)
library(mosaicData)
library(broom)
```


# 1. Inferential ideas 

How and why to perform inferential(instead of descriptive only) analysis on a regression model. 

## 1.1 Regression 

### 1.1.1 Regression example 1

```{r}
# RailTrail data: contains information about the number of users of a trail in Florence, MA and the weather for each day
data(RailTrail)
str(RailTrail)

# Fit a linear model
ride_lm <- lm(volume ~ hightemp, data = RailTrail)

# View the summary of the model
summary(ride_lm)

# Print the tidy model output
tidy(ride_lm)
```


### 1.1.2 Random samples
```{r}
# Create the test data 
popdata <- read_csv("data/popdata.csv") %>%
  separate(values, into = c("explanatory", "response" ), sep = "[:blank:]") %>%
  mutate_if(.predicate = is.character, .funs = as.numeric) %>%
  na.omit()
str(popdata)
summary(popdata)

#  Generate two samples
set.seed(4747)
both_samples <- bind_rows(
  popdata %>% sample_n(size = 50), 
  popdata %>% sample_n(size = 50), 
  .id = "replicate"
)

# plot 
both_samples %>% 
  ggplot(aes(x = explanatory, y = response, color = replicate)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)

```

### 1.1.3 Superimpose lines

```{r}
# Set the seed for reproducibility
set.seed(4747)

# Repeatedly sample the population without replacement 
many_samples <- popdata %>%
  oilabs::rep_sample_n(size = 50, reps = 100)

# see the results
glimpse(many_samples)

head(many_samples)
# Using many_samples, plot response vs explantory, grouped by replicate
ggplot(many_samples, aes(x = explanatory, y = response, group = replicate)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)


# run many lms
many_lms <- many_samples %>%
  # group by replicate
  group_by(replicate) %>%
  # Run the model on each replicate, then tidy it
  do(lm(response ~ explanatory, data = .) %>% tidy()) %>%
  # fiter for rows where the term is explanatory
  filter(term == "explanatory")

# see the result

many_lms %>%
  ggplot(aes(x = estimate)) + 
  geom_histogram()
```


## 1.2 Research question: protein & carbohydrates

Possible research questions for the starbucks data: 

* Are protein and carbohydrates linearly associated in the population? (two-sided research question)

* Are protein and carbohydrates linearly associated in a **positive** direction in the population? (one-sided research question)


## 1.3 Variability of coefficients 

### 1.3.1 Original population-change sample size

Changing the sample size directly impacts how variable the slope is. 

```{r}
set.seed(8)

# Generate 100 random samples of size 50
many_samples <- popdata %>% 
  oilabs::rep_sample_n(size = 50, reps = 100)

# Using many_samples, plot response vs explanatory, grouped by replicate
ggplot(many_samples, aes(x = response, y = explanatory, group = replicate)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)


# Generate 100 random samples of size 10
many_samples <- popdata %>% 
  oilabs::rep_sample_n(size = 10, reps = 100)

# Using many_samples, plot response vs explanatory, grouped by replicate
ggplot(many_samples, aes(x = response, y = explanatory, group = replicate)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)

```

When smaller sample size was used (10 vs 50), there was more variation in the positions of each trend line.

Reduceing the variability in the direction of the explanatory vairable **increases** the variability of the slope coefficients. This is because with a smaller range of the explanatory variables, there is less information on which to build the model. 

In summary, bigger sample size, smaller variability around the line, increased range of explanatory variable can decrease teh variability in the sampling distribution of the slope coefficient. 


# 2. Simulation-based inference for the slope parameter

learn the ideas of the sampling distribution using simulation methods for regression models. 

## 2.1 Simulation-based inference 

### 2.1.1 Null sampling distribution of the slope: twin study 

In the mid-20th century, a study was conducted that tracked down identical twins that were separated at birth: one child was raised at home of their biological parents and the other in a forster home. In an attempt to answer the question of whether intelligence is the result of nature or nurture, both children were given IQ tests. 

```{r}
# read the twins data 
twins <- read_csv("data/twins.csv")

# plot
twins %>% 
  ggplot(aes(x = Biological, y = Foster)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)

# Calculate the observed slope
obs_slope <- lm(Foster ~ Biological, data = twins) %>%
  # Tidy the result
  tidy() %>%
  # Filter for rows where term equal "Biological"
  filter(term == "Biological") %>%
  # pull out the estimate column 
  pull(estimate) 

obs_slope

# Simulate 10 slopes with a permuted dataset
perm_slope <- twins %>%
  # specify Foster vs. Biological
  specify(Foster ~ Biological) %>%
  # Use a null hypothesis of independence
  hypothesize(null = "independence") %>%
  # Generate 10 permutation replicates
  generate(reps = 10, typ = "permute") %>%
  # Calculate the slope statistic
  calculate(stat = "slope")

# see the result
perm_slope
```

Having a range of slope estiamtes will let us measure the variation and calculate confidence intervals for that statistic. 

### 2.1.2 SE of slope

```{r}
# simulate 500 slopes with a permuted dataset
perm_slope <- twins %>%
  # Specify Foster vs. Biological 
  specify(Foster ~ Biological) %>%
  # Use a null hypothesis of independence
  hypothesize(null = "independence") %>%
  # Generate 500 permutation replicates
  generate(reps = 500, type = "permute") %>%
  # Calculate the slope statistic 
  calculate(stat = "slope")

# plot 
ggplot(perm_slope, aes(x = stat)) +
  # Add a density layer
  geom_density()


# summary 
perm_slope %>%
  # ungroup the data set 
  ungroup() %>%
  summarize(
    # Mean of stat
    mean_stat = mean(stat), 
    # std error of stat
    std_err_stat = sd(stat)
  )
```

### 2.1.3 p-value

Now we have the null sampling distribution, we can use it to find the p-value associated with the original slope statistic. 

```{r}
# Run a linear reg of Foster vs. Biological on twins
abs_obs_slope <- lm(Foster ~ Biological, data = twins) %>%
  # Tidy the result
  tidy() %>%
  # Filter for rows where term equals Biological 
  filter(term == "Biological") %>%
  # Pull out the estiamte
  pull(estimate) %>%
  # Take teh absolute value
  abs()


# compute the p-value
perm_slope %>%
  # add a column of the absolute value of the slope
  mutate(abs_perm_slope = abs(stat)) %>%
  # Calculate a summary statistic 
  summarize(p_value = mean(abs_perm_slope >= abs_obs_slope) )

```


*The permutated slopes were never greater than the observed absolute slope.*   The data are unlikely under the null hypothesis assumption. 


## 2.2 Simulation-based CI for slope

### 2.2.1 Bootstrapping the data

We can repeatedly sample from the dataset to estiamte the sampling distribution and standard error of the slope coeffient. using teh sampling distribution will allow us to directly find a confidence interval for the underlying population slope. 

Bootstrap replicates samples from the data with replacement, unlike permutation replicates. 

```{r}
# Set the seed for reproducibility
set.seed(4747)

# Calculate 1000 bootstrapped slopes
boot_slope <- twins %>%
  # Specify Forster vs. Biological 
  specify(Foster ~ Biological) %>%
  # Generate 1000 bootstrap replicate
  generate(reps = 1000, type = "bootstrap") %>%
  # Calculate the slope statistic
  calculate(stat = "slope")

# see the rsult
head(boot_slope)



```


### 2.2.2 SE method-bootstrap CI for slope


```{r}
# Create a confidence interval of stat
# 2 std devs each side of the mean
boot_slope %>% 
  summarize(
    lower = mean(stat) - 2 * sd(stat),
    upper = mean(stat) + 2 * sd(stat)
  )
```


### 2.2.3 Percentile method - bootstrap CI for slope

```{r}
# Set alpha = 0.05 

alpha = 0.05 

# Set the lower percentile cutoff
p_lower <- alpha/2

# Set the upper percentile cutoff
p_upper <- 1 - alpha/2


# Create a confidence interval of stat using quantiles
boot_slope %>%
  summarize(lower = quantile(stat, p_lower), 
            upper = quantile(stat, p_upper))

```


# 3. t-Based Inference For the Slope Parameter 

In this chapter you will learn about how to use the t-distribution to perform inference in linear regression models. You will also learn about how to create prediction intervals for the response variable.

## 3.1 Mathematical approximation 

### 3.1.1 How do the theoretical results play a role? 

Instead of simulating a null distribution (using permutations), the t-distribution can be used to calculate p-values and confidence intervals. The theoretical result provides a t-distribution fit for the sampling distribution of the standardized slope statistic. 

Why does it matter if the sampling distribution is accurate? 
If the ditribution is wrong, the p-vlaue will not represent the probability of the data given the null hypothesis is true. 
If the distribution is wrong, the CI procedure will not capture the true parameter in 95% of samples. 

### 3.1.2 t-statistic

Using the permuted dataset(recall, the randomization forces the null hypothesis to be true)., investigate the distribution of the standardized slope statistics (the slope, which has been divided by the standard error). Note that the distribution of the standardized slope is well described by a t-distribution. 

```{r}
# Run a linear reg of Foster vs. Biological on twins
twins_perm <- 
  twins %>%
  specify(Foster ~ Biological) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  group_by(replicate) %>%
  do(lm(Foster ~ Biological, data = .) %>% tidy())
  

# Fiter for biological perm 
biological_perm <- twins_perm %>%
  filter(term == "Biological")

# Calculate degrees of freedom of twins
degrees_of_freedom <- nrow(twins) - 2


# Using biological_perm, plot statistic
ggplot(biological_perm, aes(x = statistic )) + 
  # add a histogram layer, with density on the y axis
  geom_histogram(aes(y = ..density..)) + 
  # Add a t-ditribution function stat, colored red
  stat_function(fun = dt, args = list(df = degrees_of_freedom), color = "red")
```

The distribution of the data (shown in the histogram) closely matches the t-distribution predicted by theory (shown by the curve). 


### 3.1.3 Working with R-output 

The p-value given by the lm output is a two-sided p-value by default. In the twin study, it might seem more reasonable to follow along the one-sided scientific hypothesis that the IQ scores of the twins are positively associated. Because the p-value is the probability of the observed data or more extreme, the two-sided test p-value is twice as big as the one-sided result. That is, to get a one-sided p-value from the two-sided output in R, divide the p-value by two.

```{r}
# build the model
model <- lm(Foster ~ Biological, data = twins) 
summary(model)

# Get the Biological model coefficient
biological_term <- model %>%
  # tidy the model
  tidy() %>%
  # filter for the term equal to "Biological"
  filter(term == "Biological")

biological_term %>% 
  # add a column of one-sided p-value   (makes the test more powerful)
  mutate(one_side_p_value = p.value/2)

```


In think about the scientific research question, if IQ is caused only by genetics, then we would expect the slope of the line between the two sets of twins to be 1. Testing the hypothesized slope of 1 can be done by taking a new test statistic which evaluates how far the observed slope is from the hypothesized values of 1. 

$$ new_t = \frac{slope-1}{SE}$$

If the hypothesis that the slope = 1 is true, then the new test statistic will have a t-ditribution that we can use for calculating a p-value. 

```{r}
# calculate new p-value
biological_term %>%
  mutate(
    # Calculate teh test statistic
    test_statistic = (estimate - 1)/std.error, 
    # calculate its one-sided p-value
    one_sided_p_value_of_test_statistic = pt(q = test_statistic, df = degrees_of_freedom), 
    # calculate its two-sided p-value
    two_sided_p_value_of_test_statistic = 2 * one_sided_p_value_of_test_statistic
  )
```

Cool calculating! The p-value of **0.31** suggests that we should not reject the null hypothesis: the slope of teh IQ line is not significantly different from **1**. 


### 3.1.4 comparing randomization inference and t-inference 
When technical conditions (see next chapter) hold, the inference from the randomization test and the t-distribution test should give equivalent conclusions. They will not provide the exact same answer because they are based on different methods. But they should give p-values and confidence intervals that are reasonably close.

```{r}

# the slope in the observed data and each permutation replicate
obs_slope <- lm(Foster ~ Biological, data = twins) %>%
  tidy() %>%
  filter(term == "Biological") %>%
  pull(estimate)

perm_slope <- twins %>%
  specify(Foster ~ Biological) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "slope")

# Calculate teh absolute value of the observed slope
abs_obs_slope <- abs(obs_slope) 

# Find the p-value
perm_slope %>%
  # add a column for the absolute value of stat
  mutate(abs_perm_slope = abs(stat)) %>%
  # Calculate prop'n permuted values more extreme than observed
  summarize(p_value = mean(abs_perm_slope > obs_slope))

```

The absolute slope estimates in the permuted datasets were never greater than the absolute slope of the observed data. 

## 3.2 Interval in Regression

### 3.2.1 CI using t-theory

In previous courses, you have created confidence intervals with the formula of statistic plus/minus some number of standard errors. With bootstrapping, we typically use two standard errors. With t-based theory, we use the specific t-multiplier.

Create a CI for the slope parameter using both the default tidy() call as well as mutate() to calculate the confidence interval bounds explicitly. Note that the two methods should give exactly the same CI values because they are using the same computations.

```{r}
# list alpha and df
alpha <- 0.05
degrees_of_freedom <- nrow(twins) - 2

# Calculate the confidence level
confidence_level <- 

```


