---
title: "Generalized Linear Models in R"
author: "Jeff Li"
date: "`r Sys.Date()`"
output: 
  html_document: 
    toc: true
    toc_depth: 3
    toc_float: true
---

"I maintained my edge by always being a student; you will always have something new to learn". - Jackie Joyner Kersee


**Course Description:**

"Linear regression serves as a workhorse of statistics, but cannot handle some types of complex data. A generalized linear model (GLM) expands upon linear regression to include non-normal distributions including binomial and count data. Throughout this course, you will expand your data science toolkit to include GLMs in R. As part of learning about GLMs, you will learn how to fit model binomial data with logistic regression and count data with Poisson regression. You will also learn how to understand these results and plot them with ggplot2."

Ref: Erickson, Richard. (2019) "Generalized Linear Models in R", www.datacamp.com. 


* Chapter 1: Review and limits of linear model and Poisson regressions

* Chapter 2: Logistic (Binomial) regression

* Chapter 3: Interpreting and plotting GLMs

* Chapter 4: Multiple regression with GLMs


## (I) Load Required Libraries
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(readxl)
library(broom)

library(lme4)

```
# 1. GLMs, an extension of your regression toolbox

This chapter teaches you how generalized linear models are an extension of other models in your data science toolbox. The chapter also uses Poisson regression to introduce generalize linear models.

## 1.1 Limitations of linear models

Linear models - $y = \beta_0 + \beta_1 x + \epsilon$: 

* Intercept for baseline effect

* Slope for linear predictor

There are several assumptions: linearity, normality, continuous variables. 

**Generalized linear model**: 

* Similar to linear models

* Non-normal error distribution

* Link functions: $y = \psi(b_0 + b_1 x + \epsilon)$


A linear model is a special case of a generalized linear model (GLM). we will use *ChickWeight* dataset to see if *diet* affects *weight*. 

```{r}
# Create the data
data("ChickWeight")
ChickWeightEnd <- ChickWeight %>% filter(Time == 21)
str(ChickWeightEnd)

# Plot
ChickWeightEnd %>%
  ggplot(aes(x = Diet, y = weight)) + 
  geom_boxplot()

# Fit a lm()
lm(formula = weight ~ Diet, data = ChickWeightEnd)

# Fit a glm()
glm(formula = weight ~ Diet, data = ChickWeightEnd, family = "gaussian")
```

## 1.2 Poisson regression

* Discrete integers: x = $0, 1, 2, 3, \dots, n$

* Mean and variance paramter: $\lambda$

* $P(x) = \frac{\lambda^x e^{-\lambda}}{x!}$

* Fixed area/time

```{r}
# Define my poisson function 
pois_fn <- function(x, lambda = 10) {
  lambda**x * exp(-lambda)/factorial(x)
}

# Plot my poisson function with "dpois" 
ggplot(data.frame(x = 0:100), aes(x)) +
  stat_function(fun = dpois, args = list(lambda = 5)) +
  stat_function(fun = dpois, args = list(lambda = 10), alpha = 0.8) +
  stat_function(fun = pois_fn, args = list(lambda = 20), color = "red", alpha = 0.8)  + 
  stat_function(fun = dpois, args = list(lambda = 30)) 
 
```


**GLM with R requirements**

* Discrete counts: 0, 1, 2, 3, ...

* Defined area and time

* Log-scale coefficients

**When not to use Poisson distribution**

* Non-count or non-positive data (e.g. 1.4 or -2)

* Non-constant sample area or time (e.g., trees/km vs trees/m)

* Mean >= 30

* Over-dispersed data

* Zero-inflated data

### 1.2.1 Fitting a Poisson regression in R

```{r}
dat <- matrix(c(
 1,     0,
 2,     0,
 3,     0,
 4,     0,
 5,     1,
 6,     0,
 7,     0,
 8,     1,
 9,     0,
10,     0,
11,     2,
12,     0,
13,     1,
14,     0,
15,     0,
16,     1,
17,     0,
18,     0,
19,     0,
20,     2,
21,     2,
22,     1,
23,     1,
24,     4,
25,     1,
26,     1,
27,     1,
28,     1,
29,     0,
30,     0), ncol = 2, byrow = TRUE)  %>% 
  as.data.frame(dat) %>% select(time = V1, count = V2)

# fit the data using the poisson family
poissonOut <- glm(count ~ time, data = dat, family = "poisson")

# print out the mode 
poissonOut

```


### 1.2.2 comparing linear and Poisson regression 
```{r}
# Fit a glm with count predicted by time using data.frame dat and gaussian family
lmOut <- glm(count ~ time, data = dat, family = "gaussian") # not a gaussina distribution 

summary(lmOut)
summary(poissonOut)
```

### 1.2.3 Intercepts-Comparisons versus means

R's formulas allow two types of intercepts to be estiamted. 

```{r}
# data
scores <- data.frame(player = rep(c("Sam", "Lou"), each = 5), 
               goal = c(1, 2, 0, 4, 3, 0, 0, 1, 0, 0))


# Fit a glm() that estimates the difference between players
summary(glm(goal ~ player, data = scores, family = "poisson"))

# Fit a glm() that estimates an intercept for each player
summary(glm(goal ~ player - 1, data = scores, family = "poisson"))

```

## 1.3 Basic lm() function s with glm()

### 1.3.1 Appying sumamry, print, and tidy to glm

```{r, eval=FALSE}
# build your models
lmOut <- lm(Number ~ Month, data = dat) 
poissonOut <- glm(Number ~ Month, data = dat, family = "poisson")

# examine the outputs using print
print(lmOut)
print(poissonOut)

# examine the outputs using summary
summary(lmOut)
summary(poissonOut)

# examine the outputs using tidy
tidy(lmOut)
tidy(poissonOut)
```

### 1.3.2 Extracting Coefficients from glm()
```{r}
# Extract the regression coefficients
coef(poissonOut)

# Extract the confidence intervals
confint(poissonOut)
```

### 1.3.3 Predicting with glm()

Recall that the Poisson slope and intercept estimates are on the natural log scale and can be exponentiated to be more easily understood. You can do this by specifying type = "response" with the predict function.


# 2. Logistic Regression 

* Binary data: (0/1)

## 2.1 Fitting a logistic regression

Pittsburgh is a city located in Allegheny County, PA, USA. Commuters living in this area have different options for commuting to work, including taking the bus. Using, data from 2015 you will see if the number of commuting days per week increases the chance somebody will use ride the bus. The choice of riding the bus is a binary outcome, hence you will use a logistic regression to model this data.

```{r}
# read the bus data
bus <- read_csv("data/busData.csv") %>% mutate_if(is.character, as.factor)

# summary 
summary(bus)

# Build a glm that models Bus predicted by CommuteDays
busOut <- glm(Bus ~ CommuteDays, data = bus, family = "binomial")
```

## 2.2 Examining logistic regression outputs

In the previous exercise, you fit a logistic regression, busOut. During this exercise, you will examine the busOut, using the tools you learned about in Chapter 1:

* print() includes the coefficient estimates (i.e., slopes and intercepts) for different predictor variables and information about the model fit such as deviance.

* summary() includes the print() outputs as well as standard errors, z-scores, and P-values for the coefficient estimates.

* tidy() includes the summary() coefficient table as a tidy data frame.

```{r}
# print the busOut
print(busOut)

# summary
summary(busOut)

# The tidy output using tidy()
tidy(busOut)
```

Interpreting logistic regression outputs: 

Notice how the trend estimate is both significantly different than zero and positive. This model tells us that commuting more days increases the chance someone takes the bus.


## 2.3 Bernoulli versus binomial

The Bernoulli distribution is a special case of the binomial with only one draw from the distribution. 


## 2.4 Simulating binary data

A *Bernoulli* distribution is a special case of a *binomial*. Next, you will see how to simulate both in R and then examine the outputs to see how they are similar. Both distributions can be simulated with the random binomial function: *rbinom()*. *rbinom()* requires 3 arguments: n, which is the number of draws or random numbers, size, which is the number of samples per draw, and prob, which is the probability for the simulation. To sample with a Bernoulli, you simply use size = 1.

If we take a single sample (size = 1) from a binomial distribution with a large sample size (e.g., n = 100), we should get similar results as a taking a many samples (e.g., size = 100) from a single sample (n = 1).

```{r}
# Simulate 1 draw with a sample size of 100
binomialSim <- rbinom(n = 1, size = 100, p = 0.5)

# Simulate 100 draw with a sample size of 1 
BernoulliSim <- rbinom(n = 100, size = 1, p = 0.5)

# Print the results from the binomial
print(binomialSim)

# Sum the results from the Bernoulli
sum(BernoulliSim)
```

Notice how the two results are similar, but not exactly the same. This is due to the inherent randomness of the simulated numbers.

## 2.5 Long-form logistic regression input

As you learned about in the video, a binomial regression can take inputs in three different formats. The first requires data to be in "long" format and directly models each observation (e.g., a response of numeric 0/1 or a factor of yes/no):

  x    y
1 a fail
2 a fail
 ...

In this format, the formula response predicted by predictor (or response ~ predictor in R's formula syntax) is used. During this exercise, you'll fit a regression using this format.

```{r}
# Fit a a long format logistic regression
lr_1 <- glm(y ~ x, data = dataLong, family = "binomial")
print(lr_1)
```

You've just fit the long dataset, which had 28 entries. You may have noticed how your degrees of freedom were 27. This is because degrees of freedom are usually the number of data points minus the number of parameters estimated.


## 2.6 Wide-form input logistic regression

The second and third approaches for fitting logistic regressions require "wide" format data:

      x fail success Total successProportion
    1 a   12       2    14         0.1428571
    2 b    3      11    14         0.7857143
    
For the second approach, model a 2 column matrix of success and failures (e.g., number of no's and yes's per group).

In this format, use the formula *cbind(success, fail) ~ predictor*.

For the third approach, model the probability of success (e.g., group 1 had 75% yes and group 2 had 65% no) and the weight or number of observations per group (e.g., there were 40 individuals in group 1 and 100 individuals in group 2).

In this format, the formula *proportion of successes ~ response variable* is used with *weights = number in treatment*.


```{r}
# Fit a wide form logistic regression
lr_2 <- glm(cbind(success, fail) ~ x, data = dataWide, family = "binomial")

# Fit a a weighted form logistic regression
lr_3 <- glm(successProportion ~ x, data = dataWide, weights = Total, family = "binomial")

# print your results
print(lr_2)
print(lr_3)
```


Comparing logistic regression outputs: 

When building models, you want to have more observations than parameters that are estimated for the model. These extra variables are called degrees of freedom. The wide versus long input formats for the glm() produce different degrees of freedom because there are different numbers of rows.

The degrees of freedom differ between models, but the other outputs are the same.


## 2.7 Probit regression 

* Both probit and logistic regression have a link function. 

* The logit model has a wider tail than the probit

* the logit model is the default for a binomial family in R


Fitting probits and logits

During this exercise, you will fit a probit and logit model to the Pittsburgh bus data. This will show you how to change link functions. After you build the models, examine the outputs. Do the results differ?

```{r}
# Fit a GLM with a logit link and save it as busLogit
busLogit <- glm(Bus ~ CommuteDays, data = bus, family = binomial(link = "logit"))

# Fit a GLM with a probit link and save it as busProbit
busProbit <- glm(Bus ~ CommuteDays, data = bus, family = binomial(link = "probit"))

# Print model summaries
summary(busLogit)

summary(busProbit)
```

Both model produce the same statistically result, but have different coefficient estimates because both models have different link functions.

## 2.8 Simulating a logit

Simulations can help us to understand distributions, evaluate our models, and compare study designs. During this exercise, you will simulate a logit distribution. This will allow to generate data with known parameters. This can be helpful when testing models or comparing study designs (e.g., how many samples do we need to collect?).

```{r}
# Using the plogis() function, convert 0 on the logit scale to a probability. Save the output as p.
p <- plogis(q = 0)

# Using the rbinom function, generate 10 samples (n = 10) with a size of 1 (size = 1) using the probability p.
rbinom(n = 10, size = 1, prob = p)
```

## 2.9 Simulating a probit

During the previous exercise, you simulated a logit. During this exercise, you will simulate a probit. First, you will convert from the probit scale to a probability. Second, you will use this probability to simulate from a binomial distribution.

```{r}
# Using function pnorm() to convert 0 on the probit scale to a probability. Save the output as p
p <- pnorm(q = 0)

# Using the rbinom function, generate 10 samples (n = 10) with a size of 1 (size = 1) using the probability p.
rbinom(n = 10, size = 1, prob = p)
```

# 3. Interpreting and visualizing GLMs

Poisson regression coefficients

