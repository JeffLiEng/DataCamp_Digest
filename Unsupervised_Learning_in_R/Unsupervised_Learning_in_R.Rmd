---
title: "Unsupervised Learning in R"
author: "Jeff Li"
date: "`r Sys.Date()`"
output: 
  html_document: 
    toc: true
    toc_depth: 3
    toc_float: true
---

"I maintained my edge by always being a student; you will always have something new to learn". - Jackie Joyner Kersee



Course Description: 

Unsupervised Learning: find patterns in data without trying to make predictions.  This course provides a basic introduction to clustering and dimentionality reduction in R from a machine learning perspective. 


Ref: Roark, Hank, Unsupervised Learning in R. www.datacamp.com, 2018.


Some course materials have been adapted for internal training. 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## (I) Load Required Libraries
```{r, message = FALSE}
library(tidyverse)

```


# 1. Unsupervised learning in R 

Objectives: 

* Learn how k-means algorithm

* Learn how to implement k-means in R, visualize and interpret the results

* Learn how to select the number of clusters and apply skills to a real world data set


**Types of machine learning**

* Unsupervised learning - (a) finding structure in unlabeled data; (b) finding homogeneous subgroups within larger group

* Supervised learning - making predictions based on labeled data (regression or classification)

* Reinforcement learning 


**Challenges and benefits of Unsupervised learning**

* No single goal of analysis 

* Requires more creativity

* Much more unlabeled data available than cleanly labeled data 


## 1.1 Introduction to k-mean clustering algorithm and k-means in R

```{r}
# (a) copy and paste txt for local practice
df_x <- read.table("data/x.txt") %>%
  select(x1 = "X..1.", 
         x2 = "X..2.")

# (b) plot
df_x %>%
  ggplot(aes(x = x1, y = x2)) +
  geom_point(alpha = 0.5)


# Create the k-means model: km_pokemon
km.out <- kmeans(df_x, centers = 3, nstart = 20)

# Inspect the result
summary(km.out)

# Print the cluster membership component of the model
table(km.out$cluster)


# Print the km.out object
# km.out

# color the cluster
df_x$cluster <- factor(km.out$cluster)  # add cluster membership to raw data

# calculate x1 and x2 mean within each group (for training only, the model already did this for us)
df_x %>%
  group_by(cluster) %>%
  summarise(x1_mean = mean(x1), 
            x2_mean = mean(x2))

df_x %>%
  ggplot(aes(x = x1, y = x2, color = cluster)) +
  geom_point(alpha = 0.5) +
  labs(title = "k-mean with 3 clusters", xlab = "", ylab = "")

# another way
# Scatter plot of x

# plot(df_x[, 1:2], col = km.out$cluster, main= "k-means with 3 clusters", xlab = "", ylab = "")
```


## 1.2 Random initialization 

*kmeans()* randomly initializes the centers of clusters, which can result in assigning observations to different cluster labels. Also, the random initialization can result in finding different local minimal for the k-means algorithm.

```{r}
# Set up 2 x 3 plotting grid
par(mfrow = c(2, 3))

# Set seed
set.seed(1)

for(i in 1:6) {
  # Run kmeans() on x with three clusters and one start
  km.out <- kmeans(df_x[, 1:2], centers = 3, nstart =1)
  
  # Plot clusters
  plot(df_x[, 1:2], col = km.out$cluster, 
       main = km.out$tot.withinss, 
       xlab = "", ylab = "")
}
```

As shown in the figure, six models have quite some variation in cluster assignments, which is due to the random initialization of the k-means algorithm. 


## 1.3 Selecting number of clusters

Due to certain business constraints, we might know the number of clusters in advance. 

If we do not know the number of clusters and need to determine it, you will need to run the algorithm multiple times, each time with a different number of clusters.

```{r}
# Initialize total within sum of squares error: wss
wss <- 0

# For 1 to 15 cluster centers
for (i in 1:15) {
  km.out <- kmeans(df_x[, 1:2], centers = i, nstart = 20)
  # Save total within sum of squares to wss variable
  wss[i] <- km.out$tot.withinss
}

# Plot total within sum of squares vs. number of clusters
plot(1:15, wss, type = "b", 
     xlab = "Number of Clusters", 
     ylab = "Within groups sum of squares")

# Set k equal to the number of clusters corresponding to the elbow location
k <- 2
```

## 1.4 Pokemon Data

```{r}
# (a) Pokemon data
pokemon_all <- read_csv("data/pokemon.csv")
summary(pokemon_all)

# (b) only select 6 dimensions 
pokemon <- pokemon_all %>%
  select(HitPoints:Speed)


# Initialize total within sum of squares error: wss
wss <- 0

# Look over 1 to 15 possible clusters
for (i in 1:15) {
  # Fit the model: km.out
  km.out <- kmeans(pokemon, centers = i, nstart = 20, iter.max = 50)
  # Save the within cluster sum of squares
  wss[i] <- km.out$tot.withinss
}

# Produce a scree plot
plot(1:15, wss, type = "b", 
     xlab = "Number of Clusters", 
     ylab = "Within groups sum of squares")

# Select number of clusters
k <- 3

# Build model with k clusters: km.out
km_pokemon <- kmeans(pokemon, centers = k, nstart = 20, iter.max = 50)

# View the resulting model
km_pokemon

# Plot of Defense vs. Speed by cluster membership
plot(pokemon[, c("Defense", "Speed")],
     col = km_pokemon$cluster,
     main = paste("k-means clustering of Pokemon with", k, "clusters"),
     xlab = "Defense", ylab = "Speed")

```


# 2. Hierarchical clustering

Objectives: 

* Understand how Hierarchical clustering works

* learn how to use Hierarchical clustering 


## 2.1 Create hierarchical clustering and cutting tree

```{r}
# (a) copy and paste txt for local practice 

hclust_x <- read.table("data/hclust_x.txt") %>%
  select(x1 = V1, 
         x2 = V2) %>%
  as.matrix()

summary(hclust_x)

class(hclust_x)

# plot data
hclust_x %>% 
  as.tibble() %>%
  ggplot(aes(x = x1, y = x2)) +
  geom_point(alpha = 0.6)

# Create hierarchical clustering model: hclust.out
x <- hclust_x
hclust.out <- hclust(d = dist(x))

# Inspect the result
summary(hclust.out)


#--- Cutting the tree -----------
# Cut by height
cutree(hclust.out, h = 7)

# Cut by number of clusters
cutree(hclust.out, k = 3)

```

The output of each *cutree()* call represents the cluster assignments for each observation in the original dataset. 


## 2.2  Linkage methods


```{r}
# Cluster using complete linkage: hclust.complete
# Complete: pairwise similarity between all observations
# in cluster 1 and cluster 2, and uses largest of similarities
hclust.complete <- hclust(dist(x), method = "complete")

# Cluster using average linkage: hclust.average
# Average: same as above but uses average of similarities
hclust.average <- hclust(dist(x), method = "average")

# Cluster using single linkage: hclust.single
# Single: same as above but uses smallest of similarities
hclust.single <- hclust(dist(x), method = "single")

# Centroid: finds centroid of cluster 1 and centroid of
# cluster 2, and uses similarity between two centroids

hclust.centroid <- hclust(dist(x), method = "centroid")


# Plot dendrogram of hclust.complete
plot(hclust.complete, main = "Complete")

# Plot dendrogram of hclust.average
plot(hclust.average, main = "Average")

# Plot dendrogram of hclust.single
plot(hclust.single, main = "Single")

# Plot dendrogram of hclust.centroid
plot(hclust.centroid, main = "Centroid")
```

**Balanced vs unbalanced trees:**

* Balanced trees: I want to an even number of observations assigned to each cluster

* unbalanced trees: I want to detect outliers (pruning an unbalanced tree can results in most obs to one cluster)


## 2.3 Scaling 

If features have different distributions, we need to scale the features. 

```{r}
# summarize data
pokemon %>%
  gather(key = variable, value = value, HitPoints:Speed) %>%
  group_by(variable) %>%
  summarise(n= n(), 
            mean = mean(value), 
            sd = sd(value)) 


# Scale the data
pokemon.scaled <- scale(pokemon)

# Create hierarchical clustering model: hclust.pokemon
hclust.pokemon <- hclust(dist(pokemon.scaled), method = "complete")


# try 3 cluster
n <- 3

pokemon$cluster <- cutree(hclust.pokemon, k = n)


# comparing kmeans() and hclust()
list(table(km = km_pokemon$cluster), 
table(hclust = pokemon$cluster), 
table(km = km_pokemon$cluster, hclust = pokemon$cluster))
```

As shown in the above tables, *hclust* assigns 788/(788+11+1) to cluster 1, while the k-means algorithm distributes the observations relatively evenly among all clusters. The analysts (we) need to make a judgment call as to which method provides more insights into the data. 



# 3. Dimensionality reduction with PCA

Objectives: 

* Learn the basic algorithm for principal component analysis (PCA)

* Understand how to use PCA to reduce data dimension

* Master how to implement PCA using R


## 3.1 PCA using prcomp()

```{r}
# select variables in pokemon
vars <- c("HitPoints", "Attack", "Defense", "Speed")
summary(pokemon[, vars])


# Perform scaled PCA: pr.out
pr.out <- prcomp(x = pokemon[, vars], scale = TRUE)

# Inspect model output
summary(pr.out)

names(pr.out)



```

The first two principal components describe around 77% of the variance. 



## 3.2 Biplot and scree plot

* **biplot()**: plots both the principal components loadings and the mapping of the observations to their first two principal component values. 

* **scree-plot**:  shows the variance explained as the number of principal components increases. 

```{r}

biplot(pr.out)

# Variability of each principal component: pr.var
pr.var <- pr.out$sdev^2

# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)
pve

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     ylim = c(0, 1), type = "b")

# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component",
     ylab = "Cumulative Proportion of Variance Explained",
     ylim = c(0, 1), type = "b")


```

The *Attack* and *HitPoint* have approximately the same loadings in the first two principal components. 


## 3.3 Practical issues with PCA 

* scaling the data

* missing values

* Categorical data

```{r}
# Mean of each variable
colMeans(pokemon)

# Standard deviation of each variable
apply(pokemon, 2, sd)

# PCA model with scaling: pr.with.scaling
pr.with.scaling <- prcomp(x = pokemon, center = TRUE, scale = TRUE)

# PCA model without scaling: pr.without.scaling
pr.without.scaling <- prcomp(x = pokemon,  scale = FALSE)

# Create biplots of both for comparison
biplot(pr.with.scaling)
biplot(pr.without.scaling)

```



# 4. Case study 

Objectives:

* Develop skills to use all unsupervised learning techniques for real-world problems

* Learn how to combine different unsupervised learning tools for your own projects


## 4.1 read data and EDA
```{r}

# read data 
wisc.df <- read.csv("data/WisconsinCancer.csv")
summary(wisc.df)
dim(wisc.df)

# How many variables/features in the data are suffixed with _mean
var_names <- names(wisc.df)
library(stringr)
library(rebus)

str_detect(var_names, pattern = "_mean" %R% END ) %>%   sum
 

# Convert the features of the data: wisc.data
wisc.data <- as.matrix(wisc.df[, 3:32])
dim(wisc.data)

# Set the row names of wisc.data
row.names(wisc.data) <- wisc.df$id

# Create diagnosis vector
diagnosis <- as.numeric(wisc.df$diagnosis == "M")

table(wisc.df$diagnosis)

```

## 4.2 Performing PCA


```{r}
# Check column means and standard deviations
colMeans(wisc.data)
apply(wisc.data, 2, sd)

# Execute PCA, scaling if appropriate: wisc.pr
wisc.pr <- prcomp(x = wisc.data,center = TRUE, scale = TRUE)

# Look at summary of results
summary(wisc.pr)
#wisc.pr$rotation

# Create a biplot of wisc.pr
biplot(wisc.pr)

# Scatter plot observations by components 1 and 2
plot(wisc.pr$x[, c(1, 2)], col = (diagnosis + 1), 
     xlab = "PC1", ylab = "PC2")

# Repeat for components 1 and 3
plot(wisc.pr$x[, c(1, 3)], col = (diagnosis + 1), 
     xlab = "PC1", ylab = "PC3")

# Do additional data exploration of your choosing below (optional)
plot(wisc.pr$x[, c(2, 3)], col = (diagnosis + 1), 
     xlab = "PC2", ylab = "PC3")

plot(wisc.pr$x[, c(1, 4)], col = (diagnosis + 1), 
     xlab = "PC1", ylab = "PC4")


# Set up 1 x 2 plotting grid
par(mfrow = c(1, 2))

# Calculate variability of each component
pr.var <- wisc.pr$sdev^2

# Variance explained by each principal component: pve
pve <- pr.var/sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "b")

# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 1), type = "b")

```

## 4.3 Hierarchical clustering of case data

```{r}
# Scale the wisc.data data: data.scaled
data.scaled <- scale(wisc.data)

# Calculate the (Euclidean) distances: data.dist
data.dist <- dist(data.scaled)

# Create a hierarchical clustering model: wisc.hclust
wisc.hclust <- hclust(d = data.dist, method = "complete")

summary(wisc.hclust)

# plot

plot(wisc.hclust)


# Cut tree so that it has 4 clusters: wisc.hclust.clusters
 wisc.hclust.clusters <- cutree(wisc.hclust, h = 20)

# Compare cluster membership to actual diagnoses
table(diagnosis, wisc.hclust.clusters)
```


## 4.4 k-means clustering and comparing results

```{r}
# Create a k-means model on wisc.data: wisc.km
# Scale the wisc.data data: data.scaled
data.scaled <- scale(wisc.data)
wisc.km <- kmeans(x = data.scaled, centers = 2, nstart = 20)

# Compare k-means to actual diagnoses
table(diagnosis, wisc.km$cluster)

# Compare k-means to hierarchical clustering
table(wisc.km$cluster, wisc.hclust.clusters)


```


## 4.5 Clustering on PCA results


```{r}
# Create a hierarchical clustering model: wisc.pr.hclust

wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method = "complete")

# Cut model into 4 clusters: wisc.pr.hclust.clusters
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k = 4)

# Compare to actual diagnoses
table(diagnosis, wisc.pr.hclust.clusters)

# Compare to k-means and hierarchical
wisc.pr.km <- kmeans(x = wisc.pr$x[, 1:7], centers = 2, nstart = 20)

table(wisc.pr.km$cluster, wisc.pr.hclust.clusters)
```









