spread(variable, mean:sd)
pokemon %>%
gather(key = variable, value = value, HitPoints:Speed) %>%
group_by(variable) %>%
summarise(mean = mean(value),
sd = sd(value))
#
pokemon %>%
gather(key = variable, value = value, HitPoints:Speed) %>%
group_by(variable) %>%
summarise(n= n(),
mean = mean(value),
sd = sd(value))
pokemon %>%
select(is.numeric)
pokemon %>%
summarise_at(.predicate = function(x) is.numeric(x),
.funs =c(Mean = "mean", Sd = "sd"))
pokemon %>%
summarise_at(.vars = function(x) is.numeric(x),
.funs =c(Mean = "mean", Sd = "sd"))
pokemon %>%
summarise_at(.vars ,
.funs =c(Mean = "mean", Sd = "sd"))
pokemon
pokemon %>%
summarise_at(.vars = HitPoints:Speed ,
.funs =c(Mean = "mean", Sd = "sd"))
pokemon
letters[1:2]
pokemon %>%
summarise_at(.vars = names() ,
.funs =c(Mean = "mean", Sd = "sd"))
pokemon %>%
summarise_at(.vars = names ,
.funs =c(Mean = "mean", Sd = "sd"))
pokemon
pokemon %>%
summarise_at(.vars = vars(HitPoints) ,
.funs =c(Mean = "mean", Sd = "sd"))
pokemon %>%
summarise_at(.vars = vars(HitPoints:Speed) ,
.funs =c(Mean = "mean", Sd = "sd"))
# Scale the data
pokemon.scaled <- scale(pokemon)
# Create hierarchical clustering model: hclust.pokemon
hclust.pokemon <- hclust(dist(pokemon.scaled), method = "complete")
# try 3 cluster
n <- 3
cluster <- cutree(hclust.pokemon, k = n)
pokemon$cluster <- cutree(hclust.pokemon, k = n)
# (a) Pokemon data
pokemon_all <- read_csv("data/pokemon.csv")
summary(pokemon_all)
# (b) only select 6 dimensions
pokemon <- pokemon_all %>%
select(HitPoints:Speed)
# Initialize total within sum of squares error: wss
wss <- 0
# Look over 1 to 15 possible clusters
for (i in 1:15) {
# Fit the model: km.out
km.out <- kmeans(pokemon, centers = i, nstart = 20, iter.max = 50)
# Save the within cluster sum of squares
wss[i] <- km.out$tot.withinss
}
# Produce a scree plot
plot(1:15, wss, type = "b",
xlab = "Number of Clusters",
ylab = "Within groups sum of squares")
# Select number of clusters
k <- 3
# Build model with k clusters: km.out
km_pokemon <- kmeans(pokemon, centers = k, nstart = 20, iter.max = 50)
# View the resulting model
km_pokemon
# Plot of Defense vs. Speed by cluster membership
plot(pokemon[, c("Defense", "Speed")],
col = km_pokemon$cluster,
main = paste("k-means clustering of Pokemon with", k, "clusters"),
xlab = "Defense", ylab = "Speed")
# comparing kmeans() and hclust()
table(km_pokemon$cluster,  pokemon$cluster)
# summarize data
pokemon %>%
gather(key = variable, value = value, HitPoints:Speed) %>%
group_by(variable) %>%
summarise(n= n(),
mean = mean(value),
sd = sd(value))
# Scale the data
pokemon.scaled <- scale(pokemon)
# Create hierarchical clustering model: hclust.pokemon
hclust.pokemon <- hclust(dist(pokemon.scaled), method = "complete")
# try 3 cluster
n <- 3
pokemon$cluster <- cutree(hclust.pokemon, k = n)
# comparing kmeans() and hclust()
table(km_pokemon$cluster,  pokemon$cluster)
# comparing kmeans() and hclust()
table(km = km_pokemon$cluster, hclust = pokemon$cluster)
# comparing kmeans() and hclust()
table(km = km_pokemon$cluster)
table(hclust = pokemon$cluster)
# comparing kmeans() and hclust()
list(table(km = km_pokemon$cluster),
table(hclust = pokemon$cluster),
table(km = km_pokemon$cluster, hclust = pokemon$cluster))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
# (a) copy and paste txt for local practice
df_x <- read.table("data/x.txt") %>%
select(x1 = "X..1.",
x2 = "X..2.")
# (b) plot
df_x %>%
ggplot(aes(x = x1, y = x2)) +
geom_point(alpha = 0.5)
# Create the k-means model: km_pokemon
km.out <- kmeans(df_x, centers = 3, nstart = 20)
# Inspect the result
summary(km.out)
# Print the cluster membership component of the model
km.out$cluster
# Print the cluster membership component of the model
table(km.out$cluster)
# Print the km.out object
km.out
# calculate x1 and x2 mean within each group (for training only, the model already did this for us)
df_x %>%
group_by(cluster) %>%
summarise(x1_mean = mean(x1),
x2_mean = mean(x2))
# color the cluster
df_x$cluster <- factor(km.out$cluster)  # add cluster membership to raw data
# calculate x1 and x2 mean within each group (for training only, the model already did this for us)
df_x %>%
group_by(cluster) %>%
summarise(x1_mean = mean(x1),
x2_mean = mean(x2))
df_x %>%
ggplot(aes(x = x1, y = x2, color = cluster)) +
geom_point(alpha = 0.5) +
labs(title = "k-mean with 3 clusters", xlab = "", ylab = "")
# Set up 2 x 3 plotting grid
par(mfrow = c(2, 3))
# Set seed
set.seed(1)
for(i in 1:6) {
# Run kmeans() on x with three clusters and one start
km.out <- kmeans(df_x[, 1:2], centers = 3, nstart =1)
# Plot clusters
plot(df_x[, 1:2], col = km.out$cluster,
main = km.out$tot.withinss,
xlab = "", ylab = "")
}
# Initialize total within sum of squares error: wss
wss <- 0
# For 1 to 15 cluster centers
for (i in 1:15) {
km.out <- kmeans(df_x[, 1:2], centers = i, nstart = 20)
# Save total within sum of squares to wss variable
wss[i] <- km.out$tot.withinss
}
# Plot total within sum of squares vs. number of clusters
plot(1:15, wss, type = "b",
xlab = "Number of Clusters",
ylab = "Within groups sum of squares")
# Set k equal to the number of clusters corresponding to the elbow location
k <- 2
# (a) Pokemon data
pokemon_all <- read_csv("data/pokemon.csv")
summary(pokemon_all)
# (b) only select 6 dimensions
pokemon <- pokemon_all %>%
select(HitPoints:Speed)
# Initialize total within sum of squares error: wss
wss <- 0
# Look over 1 to 15 possible clusters
for (i in 1:15) {
# Fit the model: km.out
km.out <- kmeans(pokemon, centers = i, nstart = 20, iter.max = 50)
# Save the within cluster sum of squares
wss[i] <- km.out$tot.withinss
}
# Produce a scree plot
plot(1:15, wss, type = "b",
xlab = "Number of Clusters",
ylab = "Within groups sum of squares")
# Select number of clusters
k <- 3
# Build model with k clusters: km.out
km_pokemon <- kmeans(pokemon, centers = k, nstart = 20, iter.max = 50)
# View the resulting model
km_pokemon
# Plot of Defense vs. Speed by cluster membership
plot(pokemon[, c("Defense", "Speed")],
col = km_pokemon$cluster,
main = paste("k-means clustering of Pokemon with", k, "clusters"),
xlab = "Defense", ylab = "Speed")
# (a) copy and paste txt for local practice
hclust_x <- read.table("data/hclust_x.txt") %>%
select(x1 = "X..1.",
x2 = "X..2.") %>%
as.matrix()
# (a) copy and paste txt for local practice
hclust_x <- read.table("data/hclust_x.txt") %>%
select(x1 = "X..1.",
x2 = "X..2.") %>%
as.matrix()
# (a) copy and paste txt for local practice
dir("data/")
read.table("data/hclust_x.txt")
hclust_x <- read.table("data/hclust_x.txt") %>%
select(x1 = V1,
x2 = V2) %>%
as.matrix()
summary(hclust_x)
class(hclust_x)
# plot data
hclust_x %>%
as.tibble() %>%
ggplot(aes(x = x1, y = x2)) +
geom_point(alpha = 0.6)
# Create hierarchical clustering model: hclust.out
x <- hclust_x
hclust.out <- hclust(d = dist(x))
# Inspect the result
summary(hclust.out)
dist(x)
# Inspect the result
summary(hclust.out)
#--- Cutting the tree -----------
# Cut by height
cutree(hclust.out, h = 7)
# Cut by number of clusters
cutree(hclust.out, k = 3)
# Cluster using complete linkage: hclust.complete
# Complete: pairwise similarity between all observations
# in cluster 1 and cluster 2, and uses largest of similarities
hclust.complete <- hclust(dist(x), method = "complete")
# Cluster using average linkage: hclust.average
# Average: same as above but uses average of similarities
hclust.average <- hclust(dist(x), method = "average")
# Cluster using single linkage: hclust.single
# Single: same as above but uses smallest of similarities
hclust.single <- hclust(dist(x), method = "single")
hclust.centroid <- hclust(dist(x), method = "centroid")
# Plot dendrogram of hclust.complete
plot(hclust.complete, main = "Complete")
# Plot dendrogram of hclust.average
plot(hclust.average, main = "Average")
# Plot dendrogram of hclust.single
plot(hclust.single, main = "Single")
# Plot dendrogram of hclust.centroid
plot(hclust.centroid, main = "Centroid")
# summarize data
pokemon %>%
gather(key = variable, value = value, HitPoints:Speed) %>%
group_by(variable) %>%
summarise(n= n(),
mean = mean(value),
sd = sd(value))
# Scale the data
pokemon.scaled <- scale(pokemon)
# Create hierarchical clustering model: hclust.pokemon
hclust.pokemon <- hclust(dist(pokemon.scaled), method = "complete")
# try 3 cluster
n <- 3
pokemon$cluster <- cutree(hclust.pokemon, k = n)
# comparing kmeans() and hclust()
list(table(km = km_pokemon$cluster),
table(hclust = pokemon$cluster),
table(km = km_pokemon$cluster, hclust = pokemon$cluster))
pokemon
pokemon[, vars]
# select variables in pokemon
vars <- c("HitPoints", "Attack", "Defense", "Speed")
pokemon[, vars]
# select variables in pokemon
vars <- c("HitPoints", "Attack", "Defense", "Speed")
pokemon[, vars]
# Perform scaled PCA: pr.out
pr.out <- prcomp(data = pokemon[, vars], scale = TRUE)
# select variables in pokemon
vars <- c("HitPoints", "Attack", "Defense", "Speed")
pokemon[, vars]
# Perform scaled PCA: pr.out
pr.out <- prcomp(x = pokemon[, vars], scale = TRUE)
# Inspect model output
summary(pr.out)
summary(pokemon[, vars])
# Perform scaled PCA: pr.out
pr.out <- prcomp(x = pokemon[, vars], scale = TRUE)
# Inspect model output
summary(pr.out)
names(pr.out)
pr.out$x
pr.out$rotation
pr.out$center
names(pr.out)
pr.out$scale
biplot(pr.out)
pokemon[430, ]
rownames(pokemon[430, ])
names(pr.out)
##
pr.var <- pr.out$sdev^2
# Variability of each principal component: pr.var
pr.var <- pr.out$sdev^2
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)
pve
# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component",
ylab = "Proportion of Variance Explained",
ylim = c(0, 1), type = "b")
# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component",
ylab = "Cumulative Proportion of Variance Explained",
ylim = c(0, 1), type = "b")
# Mean of each variable
colMeans(pokemon)
# Standard deviation of each variable
apply(pokemon, 2, sd)
# PCA model with scaling: pr.with.scaling
pr.with.scaling <- prcomp(x = pokemon, center = TRUE, scale = TRUE)
# PCA model without scaling: pr.without.scaling
pr.without.scaling <- prcomp(x = pokemon,  scale = FALSE)
# Create biplots of both for comparison
biplot(pr.with.scaling)
biplot(pr.without.scaling)
# Mean of each variable
colMeans(pokemon)
# Download the data: wisc.df
dir("data/")
# read data
wisc.df <- read.csv("data/WisconsinCancer.csv")
summary(wisc.df)
dim(wisc.df)
# read data
wisc.df <- read.csv("data/WisconsinCancer.csv")
summary(wisc.df)
dim(wisc.df)
# Convert the features of the data: wisc.data
wisc.data <- as.matrix(wisc.df[, 3:32])
# Set the row names of wisc.data
row.names(wisc.data) <- wisc.df$id
# Create diagnosis vector
diagnosis <- as.numeric(wisc.df$diagnosis == "M")
diagnosis
wisc.data
dim(wisc.data)
table(wisc.df$diagnosis)
# How many variables/features in the data are suffixed with _mean
names(wisc.data)
# How many variables/features in the data are suffixed with _mean
names(wisc.df)
library(stringr)
library(rebus)
"_mean" %R% END
str_detect(var_names, pattern = "_mean" %R% END )
# How many variables/features in the data are suffixed with _mean
var_names <- names(wisc.df)
library(stringr)
library(rebus)
str_detect(var_names, pattern = "_mean" %R% END )
str_detect(var_names, pattern = "_mean" %R% END ) %>%
sum
?apply
# Check column means and standard deviations
colMeans(wisc.data)
apply(wisc.data, 2, sd)
# Execute PCA, scaling if appropriate: wisc.pr
wisc.pr <- prcomp(x = wisc.data,center = TRUE, scale = TRUE)
# Look at summary of results
summary(wisc.pr)
# Create a biplot of wisc.pr
biplot(wisc.pr)
# Scatter plot observations by components 1 and 2
plot(wisc.pr$x[, c(1, 2)], col = (diagnosis + 1),
xlab = "PC1", ylab = "PC2")
# Repeat for components 1 and 3
plot(wisc.pr$x[, c(1, 3)], col = (diagnosis + 1),
xlab = "PC1", ylab = "PC3")
# Do additional data exploration of your choosing below (optional)
plot(wisc.pr$x[, c(2, 3)], col = (diagnosis + 1),
xlab = "PC1", ylab = "PC3")
plot(wisc.pr$x[, c(1, 4)], col = (diagnosis + 1),
xlab = "PC1", ylab = "PC3")
# Do additional data exploration of your choosing below (optional)
plot(wisc.pr$x[, c(2, 3)], col = (diagnosis + 1),
xlab = "PC2", ylab = "PC3")
plot(wisc.pr$x[, c(1, 4)], col = (diagnosis + 1),
xlab = "PC1", ylab = "PC4")
wisc.pr$rotation
# Repeat for components 1 and 3
plot(wisc.pr$x[, c(1, 3)], col = (diagnosis + 1),
xlab = "PC1", ylab = "PC3")
# Scatter plot observations by components 1 and 2
plot(wisc.pr$x[, c(1, 2)], col = (diagnosis + 1),
xlab = "PC1", ylab = "PC2")
# Repeat for components 1 and 3
plot(wisc.pr$x[, c(1, 3)], col = (diagnosis + 1),
xlab = "PC1", ylab = "PC3")
# Set up 1 x 2 plotting grid
par(mfrow = c(1, 2))
# Calculate variability of each component
pr.var <- wisc.pr$sdev^2
# Variance explained by each principal component: pve
pve <- pr.var/sum(pr.var)
# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component",
ylab = "Proportion of Variance Explained",
ylim = c(0, 1), type = "b")
# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component",
ylab = "Cumulative Proportion of Variance Explained",
ylim = c(0, 1), type = "b")
# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component",
ylab = "Proportion of Variance Explained",
ylim = c(0, 1), type = "b")
# Look at summary of results
summary(wisc.pr)
wisc.pr$rotation
# Scale the wisc.data data: data.scaled
data.scaled <- scale(wisc.data)
# Calculate the (Euclidean) distances: data.dist
data.dist <- dist(data.scaled)
# Scale the wisc.data data: data.scaled
data.scaled <- scale(wisc.data)
# Calculate the (Euclidean) distances: data.dist
data.dist <- dist(data.scaled)
# Create a hierarchical clustering model: wisc.hclust
wisc.hclust <- hclust(d = data.dist, method = "complete")
summary(wisc.hclust)
wisc.hclust
summary(wisc.hclust)
plot(wisc.hclust)
# Cut tree so that it has 4 clusters: wisc.hclust.clusters
wisc.hclust.clusters <- cutree(wisc.hclust, h = 20)
wisc.hclust.clusters
# Compare cluster membership to actual diagnoses
table(diagnosis, wisc.hclust.clusters)
plot(wisc.hclust)
# Cut tree so that it has 4 clusters: wisc.hclust.clusters
wisc.hclust.clusters <- cutree(wisc.hclust, h = 25)
# Compare cluster membership to actual diagnoses
table(diagnosis, wisc.hclust.clusters)
# Cut tree so that it has 4 clusters: wisc.hclust.clusters
wisc.hclust.clusters <- cutree(wisc.hclust, h = 20)
# Compare cluster membership to actual diagnoses
table(diagnosis, wisc.hclust.clusters)
?kmeans
# Create a k-means model on wisc.data: wisc.km
# Scale the wisc.data data: data.scaled
data.scaled <- scale(wisc.data)
wisc.km <- kmeans(x = data.scaled, centers = 2, nstart = 20)
# Compare k-means to actual diagnoses
table(diagnosis, wisc.km$centers)
# Create a k-means model on wisc.data: wisc.km
# Scale the wisc.data data: data.scaled
data.scaled <- scale(wisc.data)
wisc.km <- kmeans(x = data.scaled, centers = 2, nstart = 20)
# Compare k-means to actual diagnoses
table(diagnosis, wisc.km$centers)
wisc.km <- kmeans(x = data.scaled, centers = 2, nstart = 20)
wisc.km
# Compare k-means to actual diagnoses
table(diagnosis, wisc.km$cluster)
# Create a k-means model on wisc.data: wisc.km
# Scale the wisc.data data: data.scaled
data.scaled <- scale(wisc.data)
wisc.km <- kmeans(x = data.scaled, centers = 2, nstart = 20)
# Compare k-means to actual diagnoses
table(diagnosis, wisc.km$cluster)
# Compare k-means to hierarchical clustering
table(wisc.km$cluster, wisc.hclust.clusters)
wisc.pr
# Create a hierarchical clustering model: wisc.pr.hclust
summary(wisc.pr)
# Create a hierarchical clustering model: wisc.pr.hclust
wisc.pr.hclust <- hclus(dist(wisc.pr$x[, 1:7]), method = "complete")
# Create a hierarchical clustering model: wisc.pr.hclust
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method = "complete")
# Cut model into 4 clusters: wisc.pr.hclust.clusters
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k = 4)
# Compare to actual diagnoses
table(diagnoses, wisc.pr.hclust.clusters)
# Create a hierarchical clustering model: wisc.pr.hclust
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method = "complete")
# Cut model into 4 clusters: wisc.pr.hclust.clusters
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k = 4)
# Compare to actual diagnoses
table(diagnosis, wisc.pr.hclust.clusters)
# Compare to k-means and hierarchical
wisc.pr$x[, 1:7]
# Compare to k-means and hierarchical
wisc.pr.km <- kmean(x = wisc.pr$x[, 1:7], centers = 2, nstart = 20)
# Compare to k-means and hierarchical
wisc.pr.km <- kmeans(x = wisc.pr$x[, 1:7], centers = 2, nstart = 20)
table(wisc.pr.km$cluster, wisc.pr.hclust.clusters)
# Create a hierarchical clustering model: wisc.pr.hclust
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method = "complete")
# Cut model into 4 clusters: wisc.pr.hclust.clusters
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k = 4)
# Compare to actual diagnoses
table(diagnosis, wisc.pr.hclust.clusters)
# Compare to k-means and hierarchical
wisc.pr.km <- kmeans(x = wisc.pr$x[, 1:7], centers = 2, nstart = 20)
table(wisc.pr.km$cluster, wisc.pr.hclust.clusters)
