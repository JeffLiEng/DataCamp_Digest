cv_prep_tunerf <- cv_model_tunerf %>%
mutate(validate_actual = map(.x = validate, ~.x$life_expectancy),
validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y)$predictions))
# Calculate validate MAE for each fold and mtry combination
cv_eval_tunerf <- cv_prep_tunerf %>%
mutate(validate_mae = map2_dbl(.x = validate_actual, .y = validate_predicted, ~mae(actual = .x, predicted = .y)))
# Calculate the mean validate_mae for each mtry used
cv_eval_tunerf %>%
group_by(mtry) %>%
summarise(mean_mae = mean(validate_mae)) %>%
ggplot(aes(x = mtry, y = mean_mae)) +
geom_point() +
geom_step()
cv_eval_tunerf %>%
group_by(mtry) %>%
summarise(mean_mae = mean(validate_mae))
# Build the model using all training data and the best performing parameter
best_model <- ranger(formula = life_expectancy ~ ., data = training_data,
mtry = 3, num.trees = 500, seed = 42)
# Prepare the test_actual vector
test_actual <- testing_data$life_expectancy
# Predict life_expectancy for the testing_data
test_predicted <- predict(best_model, testing_data)$predictions
# Calculate the test MAE
mae(test_actual, test_predicted)
# (a) read *attrition* data
attrition <- readRDS("data/attrition.rds")
str(attrition)
# (a) read *attrition* data
attrition <- readRDS("data/attrition.rds")
str(attrition)
set.seed(42)
# Prepare the initial split object
data_split <- initial_split(attrition, prop = 0.75)
# Extract the training dataframe
training_data <- training(data_split)
# Extract the testing dataframe
testing_data <- testing(data_split)
cv_data
# Build a model using the train data for each fold of the cross validation
cv_models_lr <- cv_data %>%
mutate(model = map(.x = train, ~glm(formula = Attrition ~ .,
data = .x, family = "binomial")))
# (a) read *attrition* data
attrition <- readRDS("data/attrition.rds")
str(attrition)
set.seed(42)
# Prepare the initial split object
data_split <- initial_split(attrition, prop = 0.75)
# Extract the training dataframe
training_data <- training(data_split)
# Extract the testing dataframe
testing_data <- testing(data_split)
# Prepare train-test
cv_split <- vfold_cv(training_data, v = 5)
cv_data <- cv_split %>%
mutate(
# Extract the train dataframe for each split
train = map(splits, ~training(.x)),
# Extract the validate dataframe for each split
validate = map(splits, ~testing(.x))
)
# Build a model using the train data for each fold of the cross validation
cv_models_lr <- cv_data %>%
mutate(model = map(.x = train, ~glm(formula = Attrition ~ .,
data = .x, family = "binomial")))
head(cv_models_lr)
summary(cv_models_lr$model)
summary(cv_models_lr$model[[1]])
head(attrition, = 2)
head(attrition, n = 2)
names(attrition)
data_split
class(data_split)
cv_data
head(cv_models_lr)
# Build a model using the train data for each fold of the cross validation
cv_models_lr <- cv_data %>%
mutate(model = map(.x = train, ~glm(formula = Attrition ~ .,
data = .x, family = "binomial")))
head(cv_models_lr)
# Extract the first model and validate
model <- cv_models_lr$model[[1]]
validate <- cv_models_lr$validate[[1]]
# Prepare binary vector of actual Attrition values in validate
validate_actual <- validate$Attrition == "Yes"
# Predict the probabilities for the observations in validate
validate_prob <- predict(model, validate, type = "response")
# Prepare binary vector of predicted Attrition values for validate
validate_predicted <- validate_prob > 0.5
# Compare the actual & predicted performance visually using a table
table(validate_actual, validate_predicted)
# Calculate the accuracy
accuracy(validate_actual,validate_predicted)
# Calculate the precision
precision(validate_actual,validate_predicted)
# Calculate the recall
recall(validate_actual,validate_predicted)
cv_prep_lr <- cv_models_lr %>%
mutate(
# Prepare binary vector of actual Attrition values in validate
validate_actual = map(validate, ~.x$Attrition == "Yes"),
# Prepare binary vector of predicted Attrition values for validate
validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y, type = "response") > 0.5)
)
head(cv_prep_lr)
# Calculate the validate recall for each cross validation fold
cv_perf_recall <- cv_prep_lr %>%
mutate(validate_recall = map2_dbl(validate_actual, validate_predicted,
~recall(actual = .x, predicted = .y)))
# Print the validate_recall column
cv_perf_recall$validate_recall
# Calculate the average of the validate_recall column
mean(cv_perf_recall$validate_recall)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(purrr)
library(broom)
library(rsample)
library(Metrics)
library(ranger)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(purrr)
library(broom)
library(rsample)
library(Metrics)
library(ranger)
# (a) read *attrition* data
attrition <- readRDS("data/attrition.rds")
names(attrition)
# Prepare the initial split object
set.seed(42)
data_split <- initial_split(attrition, prop = 0.75)
# Extract the training dataframe
training_data <- training(data_split)
# Extract the testing dataframe
testing_data <- testing(data_split)
# Prepare train-test
cv_split <- vfold_cv(training_data, v = 5)
cv_data <- cv_split %>%
mutate(
# Extract the train dataframe for each split
train = map(splits, ~training(.x)),
# Extract the validate dataframe for each split
validate = map(splits, ~testing(.x))
)
# Build a model using the train data for each fold of the cross validation
cv_models_lr <- cv_data %>%
mutate(model = map(.x = train, ~glm(formula = Attrition ~ .,
data = .x, family = "binomial")))
head(cv_models_lr)
# Extract the first model and validate
model <- cv_models_lr$model[[1]]
validate <- cv_models_lr$validate[[1]]
# Prepare binary vector of actual Attrition values in validate
validate_actual <- validate$Attrition == "Yes"
# Predict the probabilities for the observations in validate
validate_prob <- predict(model, validate, type = "response")
# Prepare binary vector of predicted Attrition values for validate
validate_predicted <- validate_prob > 0.5
# Compare the actual & predicted performance visually using a table
table(validate_actual, validate_predicted)
# Calculate the accuracy
accuracy(validate_actual,validate_predicted)
# Calculate the precision
precision(validate_actual,validate_predicted)
# Calculate the recall
recall(validate_actual,validate_predicted)
cv_prep_lr <- cv_models_lr %>%
mutate(
# Prepare binary vector of actual Attrition values in validate
validate_actual = map(validate, ~.x$Attrition == "Yes"),
# Prepare binary vector of predicted Attrition values for validate
validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y, type = "response") > 0.5)
)
# Calculate the validate recall for each cross validation fold
cv_perf_recall <- cv_prep_lr %>%
mutate(validate_recall = map2_dbl(validate_actual, validate_predicted,
~recall(actual = .x, predicted = .y)))
# Print the validate_recall column
cv_perf_recall$validate_recall
# Calculate the average of the validate_recall column
mean(cv_perf_recall$validate_recall)
cv_data
# Prepare for tuning your cross validation folds by varying mtry
cv_tune <- cv_data %>%
crossing(mtry = c(2, 4, 8, 16))
cv_tune
# Prepare for tuning your cross validation folds by varying mtry
cv_tune <- cv_data %>%
crossing(mtry = c(2, 4, 8, 16))
# Build a cross validation model for each fold & mtry combination
cv_models_rf <- cv_tune %>%
mutate(model = map2(.x = train, .y = mtry, ~ranger(formula = Attrition~.,
data = .x, mtry = .y,
num.trees = 100, seed = 42)))
cv_models_rf
# Evaluate the validation performance
cv_models_rf %>%
mutate(map(.x = vailidate_acture, ~.x$Attrition))
cv_models_rf
# Evaluate the validation performance
cv_models_rf %>%
mutate(vailidate_acture = map_dbl(.x = validate, ~.x$Attrition))
# Evaluate the validation performance
cv_models_rf %>%
mutate(vailidate_acture = map(.x = validate, ~.x$Attrition))
# Evaluate the validation performance
cv_models_rf %>%
mutate(validate_acture = map(.x = validate, ~.x$Attrition),
validate_predicted = map2(.x = model, .y = validate, predict(.x, .y)$prediction))
# Prepare for tuning your cross validation folds by varying mtry
cv_tune <- cv_data %>%
crossing(mtry = c(2, 4, 8, 16))
# Build a cross validation model for each fold & mtry combination
cv_models_rf <- cv_tune %>%
mutate(model = map2(.x = train, .y = mtry, ~ranger(formula = Attrition~.,
data = .x, mtry = .y,
num.trees = 100, seed = 42)))
# Evaluate the validation performance
cv_models_rf %>%
mutate(validate_acture = map(.x = validate, ~.x$Attrition),
validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y)$prediction))
# Prepare for tuning your cross validation folds by varying mtry
cv_tune <- cv_data %>%
crossing(mtry = c(2, 4, 8, 16))
# Build a cross validation model for each fold & mtry combination
cv_models_rf <- cv_tune %>%
mutate(model = map2(.x = train, .y = mtry, ~ranger(formula = Attrition~.,
data = .x, mtry = .y,
num.trees = 100, seed = 42)))
# Evaluate the validation performance
cv_models_rf %>%
mutate(validate_acture = map(.x = validate, ~.x$Attrition) == "Yes",
validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y)$prediction))
# Prepare for tuning your cross validation folds by varying mtry
cv_tune <- cv_data %>%
crossing(mtry = c(2, 4, 8, 16))
# Build a cross validation model for each fold & mtry combination
cv_models_rf <- cv_tune %>%
mutate(model = map2(.x = train, .y = mtry, ~ranger(formula = Attrition~.,
data = .x, mtry = .y,
num.trees = 100, seed = 42)))
# Evaluate the validation performance
cv_models_rf %>%
mutate(validate_acture = map(.x = validate, ~.x$Attrition == "Yes") ,
validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y)$prediction))
# Evaluate the validation performance
cv_models_rf %>%
mutate(validate_acture = map(.x = validate, ~.x$Attrition == "Yes") ,
validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y, type = "response")$predictions = "Yes"))
# Evaluate the validation performance
cv_models_rf %>%
mutate(validate_acture = map(.x = validate, ~.x$Attrition == "Yes") ,
validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y, type = "response")$predictions == "Yes"))
# Evaluate the validation performance
cv_prep_rf <- cv_models_rf %>%
mutate(
# Prepare binary vector of actual Attrition values in validate
validate_actual = map(validate, ~.x$Attrition == "Yes"),
# Prepare binary vector of predicted Attrition values for validate
validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y, type = "response")$predictions == "Yes")
)
# Calculate the validate recall for each cross validation fold
cv_perf_recall <- cv_prep_rf %>%
mutate(recall = map2_dbl(.x = validate_actual, .y = validate_predicted, ~recall(actual = .x, predicted = .y)))
# Calculate the mean recall for each mtry used
cv_perf_recall %>%
group_by(mtry) %>%
summarise(mean_recall = mean(recall))
cv_data
# Prepare for tuning your cross validation folds by varying mtry
cv_tune <- cv_data %>%
crossing(mtry = c(2:20))
# Build a cross validation model for each fold & mtry combination
cv_models_rf <- cv_tune %>%
mutate(model = map2(.x = train, .y = mtry, ~ranger(formula = Attrition~.,
data = .x, mtry = .y,
num.trees = 500, seed = 42)))
# Evaluate the validation performance
cv_prep_rf <- cv_models_rf %>%
mutate(
# Prepare binary vector of actual Attrition values in validate
validate_actual = map(validate, ~.x$Attrition == "Yes"),
# Prepare binary vector of predicted Attrition values for validate
validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y, type = "response")$predictions == "Yes")
)
# Calculate the validate recall for each cross validation fold
cv_perf_recall <- cv_prep_rf %>%
mutate(recall = map2_dbl(.x = validate_actual, .y = validate_predicted, ~recall(actual = .x, predicted = .y)))
# Calculate the mean recall for each mtry used
cv_perf_recall %>%
group_by(mtry) %>%
summarise(mean_recall = mean(recall))
# Calculate the mean recall for each mtry used
cv_perf_recall %>%
group_by(mtry) %>%
summarise(mean_recall = mean(recall)) %>%
ggplot(aes(x = mtry, y = mean_recall)) +
geom_bar()
# Calculate the mean recall for each mtry used
cv_perf_recall %>%
group_by(mtry) %>%
summarise(mean_recall = mean(recall)) %>%
ggplot(aes(x = mtry, y = mean_recall)) +
geom_point()
# Calculate the mean recall for each mtry used
df <- cv_perf_recall %>%
group_by(mtry) %>%
summarise(mean_recall = mean(recall))
df
df %>%
ggplot(aes(x = mtry, y = mean_recall)) +
geom_point()
# Build the logistic regression model using all training data
best_model <- glm(formula = Attrition ~ .,
data = training_data, family = "binomial")
# Prepare binary vector of actual Attrition values for testing_data
test_actual <- testing_data$Attrition == "Yes"
# Prepare binary vector of predicted Attrition values for testing_data
test_predicted <- predict(best_model, testing_data, type = "response") > 0.5
# Compare the actual & predicted performance visually using a table
table(test_actual, test_predicted)
# Calculate the test accuracy
accuracy(test_actual, test_predicted)
# Calculate the test precision
precision(test_actual, test_predicted)
# Calculate the test recall
recall(test_actual, test_predicted)
cv_prep_lr <- cv_models_lr %>%
mutate(
# Prepare binary vector of actual Attrition values in validate
validate_actual = map(validate, ~.x$Attrition == "Yes"),
# Prepare binary vector of predicted Attrition values for validate
validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y, type = "response") )
)
cv_prep_lr
cv_prep_lr$validate_predicted[[1]]
cv_prep_lr$validate_predicted
cv_prep_lr
c(0.4:0.9)
c(50:90)
cv_prep_lr <- cv_models_lr %>%
mutate(
# Prepare binary vector of actual Attrition values in validate
validate_actual = map(validate, ~.x$Attrition == "Yes"),
# Prepare binary vector of predicted Attrition values for validate
validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y, type = "response") )
) %>%
crossing(threshold = c(50:90)/100 )
cv_prep_lr
cv_prep_lr <- cv_models_lr %>%
crossing(threshold = c(50:90)/100 ) %>%
mutate(
# Prepare binary vector of actual Attrition values in validate
validate_actual = map(validate, ~.x$Attrition == "Yes"),
# Prepare binary vector of predicted Attrition values for validate
validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y, type = "response") > threshold )
)
cv_prep_lr <- cv_models_lr %>%
crossing(threshold = c(50:90)/100 )
cv_prep_lr
cv_prep_lr <- cv_models_lr %>%
crossing(threshold = c(50:90)/100 ) %>%
mutate(
# Prepare binary vector of actual Attrition values in validate
validate_actual = map(validate, ~.x$Attrition == "Yes"),
# Prepare binary vector of predicted Attrition values for validate
validate_predicted1 = map2(.x = model, .y = validate, ~predict(.x, .y, type = "response") ),
validate_predicted = map2(.x = validate_predicted1, .y = threshold, ~.x > .y)
)
cv_prep_lr
# Calculate the validate recall for each cross validation fold
cv_perf_recall <- cv_prep_lr %>%
mutate(validate_recall = map2_dbl(validate_actual, validate_predicted,
~recall(actual = .x, predicted = .y)))
# Print the validate_recall column
cv_perf_recall$validate_recall
# Print the validate_recall column
cv_perf_recall %>%
group_by(threshold) %>%
mean(validate_recall)
# Print the validate_recall column
cv_perf_recall %>%
group_by(threshold) %>%
mean(validate_recall)
# Print the validate_recall column
cv_perf_recall %>%
group_by(threshold) %>%
summarise(mean = mean(validate_recall))
# Print the validate_recall column
cv_perf_recall %>%
group_by(threshold) %>%
summarise(mean = mean(validate_recall)) %>%
ggplot(aes(x = threshold, y = mean)) +
geom_point()
cv_prep_lr <- cv_models_lr %>%
crossing(threshold = c(40:70)/100 ) %>%
mutate(
# Prepare binary vector of actual Attrition values in validate
validate_actual = map(validate, ~.x$Attrition == "Yes"),
# Prepare binary vector of predicted Attrition values for validate
validate_predicted1 = map2(.x = model, .y = validate, ~predict(.x, .y, type = "response") ),
validate_predicted = map2(.x = validate_predicted1, .y = threshold, ~.x > .y)
)
# Calculate the validate recall for each cross validation fold
cv_perf_recall <- cv_prep_lr %>%
mutate(validate_recall = map2_dbl(validate_actual, validate_predicted,
~recall(actual = .x, predicted = .y)))
# Print the validate_recall column
cv_perf_recall %>%
group_by(threshold) %>%
summarise(validate_recall_mean = mean(validate_recall)) %>%
ggplot(aes(x = threshold, y = validate_recall_mean)) +
geom_point()
# Calculate the average of the validate_recall column
mean(cv_perf_recall$validate_recall)
cv_prep_lr <- cv_models_lr %>%
crossing(threshold = c(30:70)/100 ) %>%
mutate(
# Prepare binary vector of actual Attrition values in validate
validate_actual = map(validate, ~.x$Attrition == "Yes"),
# Prepare binary vector of predicted Attrition values for validate
validate_predicted1 = map2(.x = model, .y = validate, ~predict(.x, .y, type = "response") ),
validate_predicted = map2(.x = validate_predicted1, .y = threshold, ~.x > .y)
)
# Calculate the validate recall for each cross validation fold
cv_perf_recall <- cv_prep_lr %>%
mutate(validate_recall = map2_dbl(validate_actual, validate_predicted,
~recall(actual = .x, predicted = .y)))
# Print the validate_recall column
cv_perf_recall %>%
group_by(threshold) %>%
summarise(validate_recall_mean = mean(validate_recall)) %>%
ggplot(aes(x = threshold, y = validate_recall_mean)) +
geom_point()
cv_prep_lr <- cv_models_lr %>%
crossing(threshold = c(20:70)/100 ) %>%
mutate(
# Prepare binary vector of actual Attrition values in validate
validate_actual = map(validate, ~.x$Attrition == "Yes"),
# Prepare binary vector of predicted Attrition values for validate
validate_predicted1 = map2(.x = model, .y = validate, ~predict(.x, .y, type = "response") ),
validate_predicted = map2(.x = validate_predicted1, .y = threshold, ~.x > .y)
)
# Calculate the validate recall for each cross validation fold
cv_perf_recall <- cv_prep_lr %>%
mutate(validate_recall = map2_dbl(validate_actual, validate_predicted,
~recall(actual = .x, predicted = .y)))
# Print the validate_recall column
cv_perf_recall %>%
group_by(threshold) %>%
summarise(validate_recall_mean = mean(validate_recall)) %>%
ggplot(aes(x = threshold, y = validate_recall_mean)) +
geom_point()
cv_prep_lr <- cv_models_lr %>%
crossing(threshold = c(20:70)/100 ) %>%
mutate(
# Prepare binary vector of actual Attrition values in validate
validate_actual = map(validate, ~.x$Attrition == "Yes"),
# Prepare binary vector of predicted Attrition values for validate
validate_predicted1 = map2(.x = model, .y = validate, ~predict(.x, .y, type = "response") ),
validate_predicted = map2(.x = validate_predicted1, .y = threshold, ~.x > .y)
)
# Calculate the validate recall for each cross validation fold
cv_perf_auc <- cv_prep_lr %>%
mutate(validate_auc = map2_dbl(validate_actual, validate_predicted,
~auc(actual = .x, predicted = .y)))
# Print the validate_recall column
cv_perf_auc %>%
group_by(threshold) %>%
summarise(validate_auc_mean = mean(validate_auc)) %>%
ggplot(aes(x = threshold, y = validate_auc_mean)) +
geom_point()
cv_prep_lr <- cv_models_lr %>%
crossing(threshold = c(0:70)/100 ) %>%
mutate(
# Prepare binary vector of actual Attrition values in validate
validate_actual = map(validate, ~.x$Attrition == "Yes"),
# Prepare binary vector of predicted Attrition values for validate
validate_predicted1 = map2(.x = model, .y = validate, ~predict(.x, .y, type = "response") ),
validate_predicted = map2(.x = validate_predicted1, .y = threshold, ~.x > .y)
)
# Calculate the validate recall for each cross validation fold
cv_perf_auc <- cv_prep_lr %>%
mutate(validate_auc = map2_dbl(validate_actual, validate_predicted,
~auc(actual = .x, predicted = .y)))
# Print the validate_recall column
cv_perf_auc %>%
group_by(threshold) %>%
summarise(validate_auc_mean = mean(validate_auc)) %>%
ggplot(aes(x = threshold, y = validate_auc_mean)) +
geom_point()
# Build the logistic regression model using all training data
best_model <- glm(formula = Attrition ~ .,
data = training_data, family = "binomial")
# Prepare binary vector of actual Attrition values for testing_data
test_actual <- testing_data$Attrition == "Yes"
# Prepare binary vector of predicted Attrition values for testing_data
test_predicted <- predict(best_model, testing_data, type = "response") > 0.2
# Compare the actual & predicted performance visually using a table
table(test_actual, test_predicted)
# Calculate the test accuracy
accuracy(test_actual, test_predicted)
# Calculate the test precision
precision(test_actual, test_predicted)
# Calculate the test recall
recall(test_actual, test_predicted)
auc(test_actual, test_predicted)
# Calculate the test recall
recall(test_actual, test_predicted)
