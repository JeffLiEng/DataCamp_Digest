---
title: "Spatial Statistics in R"
author: "Jeff Li"
date: "`r Sys.Date()`"
output: 
  html_document: 
    toc: true
    toc_depth: 3
    toc_float: true
---

"I maintained my edge by always being a student; you will always have something new to learn". - Jackie Joyner Kersee


**Course Description:**

"Everything happens somewhere, and increasingly the place where all these things happen is being recorded in a database. There is some truth behind the oft-repeated statement that 80% of data have a spatial component. So what can we do with this spatial data? Spatial statistics, of course! Location is an important explanatory variable in so many things - be it a disease outbreak, an animal's choice of habitat, a traffic collision, or a vein of gold in the mountains - that we would be wise to include it whenever possible. This course will start you on your journey of spatial data analysis. You'll learn what classes of statistical problems present themselves with spatial data, and the basic techniques of how to deal with them. You'll see how to look at a mess of dots on a map and bring out meaningful insights."

Ref: Barry Rowlingson. "Spatial Statistics in R", www.datacamp.com, 2019. 


## (I) Load Required Libraries
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(spatstat)

```


# 1. Introduciton

After a quick review of spatial statistics as a whole, you'll go through some point-pattern analysis. You'll learn how to recognize and test different types of spatial patterns.

## 1.1 Simple spatial principles

Generate some point patterns: 200 points uniformly in a rectangle. 

```{r}
# The number of points to create
n <- 200

# set the range
xmin <- 0
xmax <- 1
ymin <- 0
ymax <- 2

# Sample from a Uniform distribution
x <- runif(n, 0, 1)
y <- runif(n, 0, 2)
```

## 1.2 Plotting Areas

```{r}

# Plot points and a rectangel
plot(x, y, asp = 1:1)
rect(xmin, ymin, xmax, ymax)

# define a function
mapxy <- function(a = NA) {
  plot(x, y, asp = a)
  rect(xmin, ymin, xmax, ymax)
}

mapxy(a = 1:1)

```

## 1.3 Uniform in a circle

Use *spatstat* disc() function to create a circular window. 

```{r}
# Create this many points, in a circle of this radius
n_points <- 300
radius <- 10

# Generate uniform random numbers up to radius-squared
r_squared <- runif(n_points, 0, radius*radius)
angle <- runif(n_points, 0, 2*pi)

# Take the square root of the values to get a uniform spatial distribution
x <- sqrt(r_squared) * cos(angle)
y <- sqrt(r_squared) * sin(angle)

plot(disc(radius)); points(x, y)

```




## 1.4 Quadrat count test for uniformity

Humans tend to see patterns in random arrangements, so we need statistical tests. The quadrat count test was one of the earliest developed spatial statistics methods. It can be used to check if points are completely spatially random; that is, they are uniformly random throughout the area of interest. By running a quadrat count test on the points generated in the previous exercise, you can confirm they were generated uniformly on the circle.

Quadrat count tests are implemented using quadrat.test(), which takes a planar point pattern, ppp() object. "Planar point pattern" is jargon for a set of points in a region of a 2D plane.


Given a realized point pattern from a point process in a *rectangular* region, *R*, one begins by partitioning *R* into congruent rectangular subcells (quadrats): $C_1, ..., C_m$. The CSR hypothesis asserts that the cell-count distribution for each $C_i$ must be the same. But rather than use Binomial distribution, it is typically assumed that *R* is large enough to use the Poisson approximation. If there are n points in R, and if we let a = a(C1), and estimate expected point density $\lambda$ by: 

$$\lambda = \frac{n}{a(R)}$$

Then this common *Poisson cell-count distribution* has the form: 

$$Pr[N_i = k|\lambda] = \frac{(\lambda a)^k}{k!}e^{-\lambda a}$$

Moreover, since the CSR Hypothesis also implies that each of the cell counts, $N_i = N(C_i), i = 1, ..., k$, is independent. It follows that (Ni: i = 1, ..., k) must be an independent random samples from this Poisson distribution. Hence the simplest test of this hypothesis is to use Pearson $\chi ^2$ goodness-of-fit test. Here the expected number of points in each cell is given by the mean of Poisson above. 

$$E(N|\lambda) = a \lambda = a \frac{n}{a(R)} = \frac{n}{m}$$


Hence if the observed value of $N_i$ is denoted by $n_i$, then the chi-square statistic: 

$$\chi ^2 = \sum_i^m \frac{(n_i-n/m)^2}{n/m}$$

is known to be asymptotically chi-square distributed with m-1 degrees of freedom. But since n/m is simple the sample mean, this statistic can also be written as: 

$$chi ^2 = \sum_i^m \frac{(n_i - \overline n)^2}{\overline n} = (m-1)\frac{s^2}{\overline n}$$

If $s^2/\overline n < 1$ then there is too little variation among quadrat count, suggesting possible "dispersion" rather than randomness. Similiarly, if $s^2/\overline n >1 $ then there is too much variations among counts, suggesting possible "clustering" rather than randomness. 



```{r}
# Set coordinates and window
ppxy <- ppp(x = x, y = y, window = disc(radius))
summary(ppxy)

# Test the point pattern
qt <- quadrat.test(X = ppxy)

# Inspect the results
plot(qt)
print(qt)


```

The p-value of the quadrat test is much bigger than 0.05, so you can not reject the hypothesis that the points are completely spatially random.


## 1.5 Creating a uniform point pattern with spatstat
A Poisson point process creates events according to a Poisson distribution with an intensity parameter specifying the expected events per unit area. The total number of events generated is a single number from a Poisson distribution, so multiple realisations of the same process can easily have different numbers of events.

In the previous exercise you used a set of 300 events scattered uniformly within a circle. If you repeated the generation of the events again you will still have 300 of them, but in different locations. The dataset of exactly 300 points is from a Poisson point process conditioned on the total being 300.

The spatstat package can generate Poisson spatial processes with the rpoispp() function given an intensity and a window, that are not conditioned on the total.

Just as the random number generator functions in R start with an "r", most of the random point-pattern functions in spatstat start with an "r".

The area() function of spatstat will compute the area of a window such as a disc.

```{r}
# Create a disc of radius 10
disc10 <-  disc(10)

# Compute the rate as count divided by area
lambda <- 500/area(disc10)

# Create a point pattern object
ppois <- rpoispp(lambda = lambda, win = disc10)

# Plot the Poisson point pattern 
plot(ppois)

# Test the point pattern
quadrat.test(X = ppois)
```

Perfect Poisson process plotting! Poisson processes generate completely spatially random points. Next you'll see some other processes for generating random points.


## 1.6 Simulating clustered and inhibitory patterns

The spatstat package also has functions for generating point patterns from other process models. These generally fall into one of two classes: clustered processes, where points occur together more than under a uniform Poisson process, and regular (aka inhibitory) processes where points are more spaced apart than under a uniform intensity Poisson process. Some process models can generate patterns on a continuum from clustered through uniform to regular depending on their parameters.

The quadrat.test() function can test against clustered or regular alternative hypotheses. By default it tests against either of those, but this can be changed with the alternative parameter to create a one-sided test.

A Thomas process is a clustered pattern where a number of "parent" points, uniformly distributed, create a number of "child" points in their neighborhood. The child points themselves form the pattern. This is an attractive point pattern, and makes sense for modeling things like trees, since new trees will grow near the original tree. Random Thomas point patterns can be generated using rThomas(). This takes three numbers that determine the intensity and clustering of the points, and a window object.

Conversely the points of a Strauss process cause a lowering in the probability of finding another point nearby. The parameters of a Strauss process can be such that it is a "hard-core" process, where no two points can be closer than a set threshold. Creating points from this process involves some clever simulation algorithms. This is a repulsive point pattern, and makes sense for modeling things like territorial animals, since the other animals of that species will avoid the territory of a given animal. Random Strauss point patterns can be generated using rStrauss(). This takes three numbers that determine the intensity and "territory" of the points, and a window object. Points generated by a Strauss process are sometimes called regularly spaced.

```{r}
# Create a disc of radius 10
disc10 <- disc(radius = 10)

# Generate clustered points from a Thomas process
set.seed(123)
p_cluster <- rThomas(
  # Intensity of the Poisson process of cluster centres
  kappa = 0.35, 
  # Standard deviation of random displacement
  scale = 1, 
  # mean number of points per cluster
  mu = 3,
  # Window in which to simulate the pattern
  win = disc10)

plot(p_cluster)

# Run a quadrat test
quadrat.test(X = p_cluster, alternative = "clustered")

# Regular points from a Strauss process
set.seed(123)
p_regular <- rStrauss(
  # intensity parameter (positive)
  beta = 2.9, 
  # Interaction parameter
  gamma = 0.025, 
  # Interaction radius
  R = 0.5, 
  # Window in which to generate the random pattern
  W = disc10)

plot(p_regular)

# Run a quadrat test
quadrat.test(X = p_regular, 
             alternative = "regular")

```

Thomas and Strauss processes are important, complementary ways of generating point patterns common in nature.


## 1.7 Nearest-Neighbor Methods

The quadrat method procedure is very restrictive in that it requires an equal-area partition of the given region. It also depends critically on the *size* of the partition chosen.  Nearest-Neighbor methods is based on the observation that if one simply looks at distances between points and their nearest neighbor in R, then this provides a natural test statistic that requires no artificail partitioning scheme. More precisely, for any given points, $s = (s_1, s_2)$ and $v = (v_1, v_2)$ in R, we denote the Euclidean distance between s and v by: 

$$ d(s,v) = \sqrt{(s_1 - v_1)^2 + (s_2 - v_2)^2}$$


One simple measure is the distribution of the distances from each point to its nearest neighbor.

Another way of assessing clustering and regularity is to consider each point, and how it relates to the other points. One simple measure is the distribution of the distances from each point to its nearest neighbor.

The nndist() function in spatstat takes a point pattern and for each point returns the distance to its nearest neighbor. You can then plot the histogram.

Instead of working with the nearest-neighbor density, as seen in the histogram, it can be easier to work with the cumulative distribution function, G(r). This is the probability of a point having a nearest neighbour within a distance r.

For a uniform Poisson process, G can be computed theoretically, and is G(r) = 1 - exp( - lambda * pi * r ^ 2). You can compute G empirically from your data using Gest() and so compare with the theoretical value.

Events near the edge of the window might have had a nearest neighbor outside the window, and so unobserved. This will make the distance to its observed nearest neighbor larger than expected, biasing the estimate of G. There are several methods for correcting this bias.

Plotting the output from Gest shows the theoretical cumulative distribution and several estimates of the cumulative distribution using different edge corrections. Often these edge corrections are almost indistinguishable, and the lines overlap. The plot can be used as a quick exploratory test of complete spatial randomness.


```{r}
# Create a disc of radius 10
disc10 <-  disc(10)

# Compute the rate as count divided by area
lambda <- 500/area(disc10)

# Create a point pattern object
p_poisson <- rpoispp(lambda = lambda, win = disc10)

# Regular points from a Strauss process
set.seed(123)
p_regular <- rStrauss(
  # intensity parameter (positive)
  beta = 2.9, 
  # Interaction parameter
  gamma = 0.025, 
  # Interaction radius
  R = 0.5, 
  # Window in which to generate the random pattern
  W = disc10)

# Calc nearest-neighbor distances for poisson point data 
nnd_poisson <- nndist(p_poisson)

# Draw a histogram of nearest-neighbor distance
hist(nnd_poisson)

# Estimate G(r)
G_poisson <- Gest(X = p_poisson)

# Plot (G(r) vs. R)

plot(G_poisson )


# Calc nearest-neighbor distances for regular point data 
nnd_regular <- nndist(p_regular)

# Draw a histogram of nearest-neighbor distance
hist(nnd_regular)

# Estimate G(r)
G_regular <- Gest(X = p_regular)

# Plot (G(r) vs. R)

plot(G_regular)

```

Notice how G for the regular point pattern stays low at small distance, indicating very low probabilities of finding close pairs of points.

## 1.8 Other point pattern distribution functions

A number of other functions of point patterns have been developed. They are conventionally denoted by various capital letters, including F, H, J, K and L.

The K-function is defined as the expected number of points within a distance of a point of the process, scaled by the intensity. Like G, this can be computed theoretically for a uniform Poisson process and is K(r) = pi * r ^ 2 - the area of a circle of that radius. Deviation from pi * r ^ 2 can indicate clustering or point inhibition.

Computational estimates of K(r) are done using the Kest() function.

As with G calculations, K-function calculations also need edge corrections. The default edge correction in spatstat is generally the best, but can be slow, so we'll use the "border" correction for speed here.

Uncertainties on K-function estimates can be assessed by randomly sampling points from a uniform Poisson process in the area and computing the K-function of the simulated data. Repeat this process 99 times, and take the minimum and maximum value of K over each of the distance values. This gives an envelope - if the K-function from the data goes above the top of the envelope then we have evidence for clustering. If the K-function goes below the envelope then there is evidence for an inhibitory process causing points to be spaced out. Envelopes can be computed using the envelope() function.

The plot method for estimates of K uses a formula system where a dot on the left of a formula refers to K(r). So the default plot uses . ~ r. You can compare the estimate of K to a Poisson process by plotting . - pi * r ^ 2 ~ r. If the data was generated by a Poisson process, then the line should be close to zero for all values of r.


Define the *ppp* objects of *p_poisson*, *p_cluster*, and *p_regular*. 
```{r}
# Create a disc of radius 10
disc10 <-  disc(10)

# Compute the rate as count divided by area
lambda <- 500/area(disc10)

# Create a point pattern object
p_poisson <- rpoispp(lambda = lambda, win = disc10)

# Regular points from a Strauss process
set.seed(123)
p_regular <- rStrauss(
  # intensity parameter (positive)
  beta = 2.9, 
  # Interaction parameter
  gamma = 0.025, 
  # Interaction radius
  R = 0.5, 
  # Window in which to generate the random pattern
  W = disc10)


# Regular points from a Strauss process
set.seed(123)
p_regular <- rStrauss(
  # intensity parameter (positive)
  beta = 2.9, 
  # Interaction parameter
  gamma = 0.025, 
  # Interaction radius
  R = 0.5, 
  # Window in which to generate the random pattern
  W = disc10)
```

```{r}
# Estimate the K-function for the Poisson points
K_poisson <- Kest(X = p_poisson, correction = "border")

# The Default plot shows quadratic growth
plot(K_poisson, . ~ r)

# plot the K function with a formular that substracts the theoretical Poisson value
plot(K_poisson, - pi*r^2 ~ r)

# compute envelopes of K under random locations
K_cluster_env <- envelope(p_cluster, fun = Kest, correction = "border")

# Insert the full formula to plot K minus pi * r^2
plot(K_cluster_env, . -pi * r^2 ~ r)

# compute envelops of K under regular locations
K_regular_env <- envelope(p_regular, fun = Kest, correction = "border")

plot(K_regular_env, .-pi*r^2 ~ r)
```

 Envelopes are useful where the theoretical distribution is hard or impossible to calculate.


# 2. Point Pattern Analysis
Point Pattern Analysis answers questions 










# 4. Geostatistics

Originally developed for the mining industry, geostatistics covers the analysis of location-based measurement data. It enables model-based interpolation of measurement with uncertainty estimation. 

## 4.1 Canadian geochemical survey data

Study the acidity (pH) of some Canadian survey data. 

```{r}
# Read the data: spatial data object 
ca_geo <- read_rds("data/ca_geo.rds")

# structure of the data
str(ca_geo)

# ca_geo has been pre-defined
str(ca_geo, 1)

# See what measurements are at each location
names(ca_geo)

# Get a summary of the acidity (pH) values
summary(ca_geo$pH)

# Look at the distribution
hist(ca_geo$pH)

# Make a vector that is TRUE for the missing data
miss <- is.na(ca_geo$pH)
table(miss)

# Plot a map of acidity
spplot(ca_geo[!is.na(ca_geo$pH), ], "pH")
```


## 4.2 Fitting a trend surface

The acidity data shows pH broadly increasing from north-east to south-west. Fitting a linear model with the coordinates as covariates will interpolate a flat plane through the values.

```{r}
# coordinate names
coordnames(ca_geo)

# Fitting a trend surface
m_trend <- lm(pH ~ x + y, as.data.frame(ca_geo))

# Check the coefficients
summary(m_trend)

```

Lovely linear modeling! Linear regressions are a good first model for exploring many datasets. 


## 4.3 Predicting from a trend surface

Your next task is to compute the pH at the locations that have missing data in the source. You can use the predict() function on the fitted model from the previous exercise for this.

```{r}
# Make a vector that is TRUE for the missing data
miss <- is.na(ca_geo$pH)

# Create a data frame of missing data
ca_geo_miss <- as.data.frame(ca_geo)[miss, ]

# Predict pH for the missing data
predictions <- predict(m_trend, newdata = ca_geo_miss, se.fit = TRUE)

# Compute the exceedence probability
pAlkaline <- 1 - pnorm(7, mean = predictions$fit, sd = predictions$se.fit)
hist(pAlkaline)
```

## 4.4 Variogram Estimation 

You can use the gstat package to plot variogram clouds and the variograms from data. Recall:

* The variogram cloud shows the differences of the measurements against distance for all pairs of data points.

* The binned variogram divides the cloud into distance bins and computes the average difference within each bin.

The y-range of the binned variogram is always much smaller than the variogram cloud because the cloud includes the full range of values that go into computing the mean for the binned variogram.

```{r}
# Make a cloud from the non-missing data up to 10km
plot(gstat::variogram(pH ~ 1, ca_geo[!miss, ], cloud = TRUE, cutoff = 10000))

# Make a variogram of the non-missing data
plot(gstat::variogram(pH ~ 1, ca_geo[!miss, ]))
```

Measurements from sites that are further away from each other are more different. 


## 4.5 Variagram with spatial trend

You might imagine that if soil at a particular point is alkaline, then soil one metre away is likely to be alkaline too. But can you say the same thing about soil one kilometre away, or ten kilometres, or one hundred kilometres?

The shape of the previous variogram tells you there is a large-scale trend in the data. You can fit a variogram considering this trend with gstat. This variogram should flatten out, indicating there is no more spatial correlation after a certain distance with the trend taken into account.

```{r}
# The pH depends on the coordinates
ph_vgm <- gstat::variogram(pH ~ x + y, ca_geo[!miss, ])
plot(ph_vgm)
```

The plot levels off after around 25000m, indicating that there appears to be little spatial correlation beyond that distance.

## 4.6 variogram model fitting

Next you'll fit a model to your variogram. The gstat function fit.variogram() does this. You need to give it some initial values as a starting point for the optimization algorithm to fit a better model.

The sill is the the upper limit of the model. That is, the long-range largest value, ignoring any outliers.


A variogram has been plotted for you, and ph_vgm has been pre-defined.

* Estimate some parameters by eyeballing the plot.

* The nugget is the value of the semivariance at zero distance.

* The partial sill, psill is the difference between the sill and the nugget.

* Set the range to the distance at which the variogram has got about half way between the nugget and the sill.

* Fit a variogram model by calling fit.variogram().

* The second argument should take the parameters you estimated, wrapped in a call to vgm().
Plot the binned variogram.

```{r}
# Eyeball the variogram and estimate the initial parameters
nugget <- 0.16
psill <-  0.12
range <- 10000

# Fit the variogram
v_model <- gstat::fit.variogram(
  ph_vgm, 
  model = gstat::vgm(
    model = "Ste",
    nugget = nugget,
    psill = psill,
    range = range,
    kappa = 0.5
  )
)

# Show the fitted variogram on top of the binned variogram
plot(ph_vgm, model = v_model)
print(v_model)
```

The model prediction helps you ignore measurement errors to more easily see the distance at which spatial correlation no longer occurs.


## 4.7 Filling in the gaps

The final part of geostatical estimation is kriging itself. This is the application of the variogram along with the sample data points to produce estimates and uncertainties at new locations.

The computation of estimates and uncertainties, together with the assumption of a normal (Gaussian) response means you can compute any function of the estimates - for example the probability of a new location having alkaline soil.


```{r}
# Set the trend formula and the new data
km <- gstat::krige(pH ~ x + y, ca_geo[!miss, ], newdata = ca_geo[miss, ], model = v_model)
names(km)

# Plot the predicted values
spplot(km, "var1.pred")

# Compute the probability of alkaline samples, and map
km$pAlkaline <- 1 - pnorm(7, mean = km$var1.pred, sd = sqrt(km$var1.var))
spplot(km, "pAlkaline")
```

Next you'll see how to go from points predictions to gridded predictions.


## 4.8 Making a prediction grid

You have been asked to produce an alkaline probability map over the study area. To do this, you are going to do some kriging via the *krige()* function. This requires a *SpatialPixels* object which will take a bit of data manipulation to create. You start by defining a grid, creating points on that grid, cropping to the study region, and then finally converting to *SpatialPixels*. On the way, you'll meet some new functions.

*GridTopology()* defines a rectangular grid. It takes three vectors of length two as inputs. The first specifies the position of the bottom left corner of the grid. The second specifies the width and height of each rectangle in the grid, and the third specifies the number of rectangles in each direction.

To ensure that the grid and the study area have the same coordinates, some housekeeping is involved. *SpatialPoints()* converts the points to a coordinate reference system (CRS), or projection (different packages use different terminology for the same concept). The CRS is created by wrapping the study area in *projection()*, then in CRS(). For the purpose of this exercise, you don't need to worry about exactly what these functions do, only that this data manipulation is necessary to align the grid and the study area.

Now that you have that alignment, *crop()*, as the name suggests, crops the grid to the study area.

Finally, *SpatialPixels()* converts the raster cropped gridpoints to the equivalent sp object.


```{r, eval=FALSE}

# Plot the polygon and points
plot(geo_bounds); points(ca_geo)

# Find the corners of the boundary
bbox(geo_bounds)

# Define a 2.5km square grid over the polygon extent. The first parameter is
# the bottom left corner.
grid <- GridTopology(c(537853, 5536290), c(2500, 2500), c(72, 48))

# Create points with the same coordinate system as the boundary
gridpoints <- SpatialPoints(grid, proj4string = CRS(projection(geo_bounds)))
plot(gridpoints)

# Crop out the points outside the boundary
cropped_gridpoints <- crop(gridpoints, geo_bounds)
plot(cropped_gridpoints)

# Convert to SpatialPixels
spgrid <- SpatialPixels(cropped_gridpoints)
coordnames(spgrid) <- c("x", "y")
plot(spgrid)
```

## 4.9 Gridded predictions

Constructing the grid is the hard part done. You can now compute kriged estimates over the grid using the variogram model from before (v_model) and the grid of SpatialPixels.


```{r}
# spgrid, v_model have been pre-defined
ls.str()

# Do kriging predictions over the grid
ph_grid <- krige(pH ~ x + y, ca_geo[!miss, ], newdata = spgrid, model = v_model)

# Calc the probability of pH exceeding 7
ph_grid$pAlkaline <- 1 - pnorm(7, mean = ph_grid$var1.pred, sd = sqrt(ph_grid$var1.var))

# Map the probability of alkaline samples
spplot(ph_grid, zcol = "pAlkaline")
```

The regions with alkaline samples are much clearer than in the maps of points that you saw at the start of the chapter.


## 4.10 Auto-kriging at point locations

The *autoKrige()* function in the *automap* package computes binned variograms, fits models, does model selection, and performs kriging by making multiple calls to the *gstat* functions you used previously. It can be a great time-saver but you should always check the results carefully.

In this example you will get predictions at the missing data locations.

*autoKrige()* can try several variogram model types. In the example, you'll use a Matern variogram model, which is commonly used in soil and forestry analyses. You can see a complete list of available models by calling *vgm()* with no arguments.


```{r}
# Kriging with linear trend, predicting over the missing points
ph_auto <- automap::autoKrige(
  pH ~ x + y, 
  input_data = ca_geo[!miss, ], 
  new_data = ca_geo[miss, ], 
  model = "Mat"
)

# Plot the variogram, predictions, and standard error
plot(ph_auto)
```

Notice that the cluster in the East is predicted to be slightly acidic, and the cluster in the West alkaline.


## 4.11 Auto-kriging over a grid

You can also use *autoKrige()* over the spgrid grid from the earlier exercise. This brings together all the concepts that you've learned in the chapter. That is, kriging is great for predicting missing data, plotting things on a grid is much clearer than plotting individual points, and automatic kriging is less hassle than manual kriging.

```{r}
# ca_geo, miss, spgrid, ph_grid, v_model are pre-defined
ls.str()

# Auto-run the kriging
ph_auto_grid <- autoKrige(pH ~ x + y, input_data = ca_geo[!miss, ], new_data = spgrid)

# Remember predictions from manual kriging
plot(ph_grid)

# Plot predictions and variogram fit
plot(ph_auto_grid)

# Compare the variogram model to the earlier one
v_model
ph_auto_grid$var_model
```

Automated kriging gives you a lot of modeling power for not much effort.