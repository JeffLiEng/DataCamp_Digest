---
title: "Introduction to Time Series Analysis"
author: "Jeff Li"
date: "`r Sys.Date()`"
output: 
  html_document: 
    toc: true
    toc_depth: 3
    toc_float: true
---

"I maintained my edge by always being a student; you will always have something new to learn". - Jackie Joyner Kersee


**Course Description**

"Many phenomena in our day-to-day lives, such as the movement of stock prices, are measured in intervals over a period of time. Time series analysis methods are extremely useful for analyzing these special data types. In this course, you will be introduced to some core time series analysis concepts and techniques."

Ref: matteson, David (2018) Introduction to Time Series Analysis, https://www.datacamp.com/courses/introduction-to-time-series-analysis, 2018.


Note: Some course materials and data have beem revised for training by Jeff Li. 

# (I) Load required libraries
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)

```


# 1. Exploratory time series data analysis

"This chapter will give you insights on how to organize and visualize time series data in R. You will learn several simplifying assumptions that are widely used in time series analysis, and common characteristics of financial time series."

## 1.1 Exloring raw time series

River Nile annual streamflow data. 

```{r, eval=FALSE}
# create the data
 streamflow <- c(1120, 1160,  963, 1210, 1160, 1160,  813, 1230, 1370, 1140,  995,  935, 1110,  994, 1020,
                  960, 1180,  799,  958, 1140, 1100, 1210, 1150, 1250, 1260, 1220, 1030, 1100,  774,  840,
                  874,  694,  940,  833,  701,  916,  692, 1020, 1050,  969,  831,  726,  456,  824,  702,
                 1120, 1100,  832,  764,  821,  768,  845,  864,  862,  698,  845,  744,  796, 1040,  759,
                  781,  865,  845,  944,  984,  897,  822, 1010,  771,  676,  649,  846,  812,  742,  801,
                 1040,  860,  874,  848,  890,  744,  749,  838, 1050,  918,  986,  797,  923,  975,  815,
                 1020,  906,  901, 1170,  912,  746,  919,  718,  714,  740)

Nile <- ts(streamflow, frequency = 1, start = 1871, end = 1970)

# print the Nile dataset
print(Nile)

# List the number of observation in the Nile dataset
length(Nile)

# Display the first 10 elelments of the Nile dataset
head(Nile, n = 10)

# Display the last 12 elements of the Nile dataset
tail(Nile, n = 12)
```

## 1.2 Basic time series plots

```{r}
# Plot the Nile data
plot(Nile)

# Plot the Nile data with xlab adn ylab arguments
plot(Nile, xlab = "Year", ylab = "River Volume (1e9 m^{3}")

# Plot the Nile data with xlab, ylab, main, and type arguments
plot(Nile, xlab = "Year", ylab = "River Volume (1e9 m^{3}", main = "Annual River Nile Volume at Aswan, 1871 - 1970", type = "b")
```

## 1.3 Continuous and discrete time index 

```{r, eval=FALSE}
# Plot the continuous_series using continuous time indexing
par(mfrow=c(2,1))
plot(continuous_time_index,continuous_series, type = "b")

# Make a discrete time index using 1:20 
discrete_time_index <- 1:20

# Now plot the continuous_series using discrete time indexing
plot(discrete_time_index,continuous_series, type = "b")

```

## 1.4 Sampling Frequency

* Exact 

* Approximate

* Missing

R Functions: start(), end(), frequency(), deltat(). There functions can provide considerable descriptive information about the structures and patterns in the time series data. 

```{r, eval=FALSE}
# Plot AirPassengers
plot(AirPassengers)

# View the start and end dates of AirPassengers
start(AirPassengers)
end(AirPassengers)


# Use time(), deltat(), frequency(), and cycle() with AirPassengers 
time(AirPassengers)
deltat(AirPassengers)
frequency(AirPassengers)
cycle(AirPassengers)

```

## 1.5 Missing values

Normally, a simple data imputation using the mean is not a great method to approximate what's really going on in the real situation. 

```{r, eval=FALSE}
# Plot the AirPassengers data
plot(AirPassengers)

# Compute the mean of AirPassengers
mean(AirPassengers, na.rm = TRUE)

# Impute mean values to NA in AirPassengers
AirPassengers[85:96] <- mean(AirPassengers, na.rm = TRUE)

# Generate another plot of AirPassengers
plot(AirPassengers)

# Add the complete AirPassengers data to your plot
rm(AirPassengers)
points(AirPassengers, type = "l", col = 2, lty = 3)

```

## 1.6 Basic Time Series Objects

```{r}
# building ts() objects
data_vector <- c(10, 6, 11, 8, 10, 3, 6, 9)

time_series <- ts(data_vector)
plot(time_series)

# specifiy the start date and observation frequency
time_series <- ts(data_vector, start = 2001, frequency = 1)
plot(time_series)

# check whether an object is of the ts() class: 
is.ts(data_vector)
is.ts(time_series)
```

A time series object is a vector (univariate) or matrix with addition attributes: time indices, sampling frequency, time increment, the cycle length. 

```{r, eval=FALSE}
# Use print() and plot() to view data_vector
print(data_vector)
plot(data_vector)

# Convert data_vector to a ts object with start = 2004 and frequency = 4
time_series <- ts(data_vector, start = 2004, frequency = 4)

# Use print() and plot() to view time_series
print(time_series)
plot(time_series)
  
```

## 1.7 Plotting a time series object
```{r}
# Check whether eu_stocks is a ts object
eu_stocks <- EuStockMarkets
is.ts(eu_stocks)

# View the start, end, and frequency of eu_stocks
start(eu_stocks)
end(eu_stocks)
frequency(eu_stocks)

# Generate a simple plot of eu_stocks
plot(eu_stocks)

# Use ts.plot with eu_stocks
ts.plot(eu_stocks, col = 1:4, xlab = "Year", ylab = "Index Value", main = "Major European Stock Indices, 1991-1998")

# Add a legend to your ts.plot
legend("topleft", colnames(eu_stocks), lty = 1, col = 1:4, bty = "n")
```


# 2. Predicting the future

## 2.1 Trend Spotting

* Linear     (diff() can remove a linear trend)

* Rapid growth    (log transformation can linearize a rapid growth trend)

* Periodic

* Variance: increase variance trends over time


## 2.2 Removing trends in level by differencing

The first difference transformation of a time series z[t] consists of the differences (changes) between successive observations over time, that is z[t]−z[t−1].

Differencing a time series can remove a time trend. The function diff() will calculate the first difference or change series. A difference series lets you examine the increments or changes in a given time series. It always has one fewer observations than the original series.

```{r, eval=FALSE}
# Generate the first difference of z
dz <- diff(z)
  
# Plot dz
ts.plot(dz)

# View the length of z and dz, respectively
length(z)
length(dz)
```
By removing the long-term time trend, we can view the amount of change from one observation to the next. 


## 2.3 Removing seasonal trends with seasonal differencing
```{r, eval=FALSE}
# Generate a diff of x with lag = 4. Save this to dx
dx <- diff(x, lag = 4)
  
# Plot dx
  ts.plot(dx)

# View the length of x and dx, respectively 
length(x)
length(dx)


```


## 2.4 Simulate the white noise model
The white noise (WN) model is a basic time series model. 

The **arima.sim()** function can be used to simulated data from a variety of time series models. ARIMA: autoregressive integrated moving average class of model. 

```{r}
# Simulate a WN model with list(order = c(0, 0, 0))
white_noise <- arima.sim(model = list(order= c(0,0,0)), n = 100)

# Plot the white noise data
ts.plot(white_noise)

# Simulate form the WN moel with: mean = 100, sd = 10
white_noise_2 <- arima.sim(model = list(order = c(0,0,0)), n = 100, mean = 100, sd = 10)

# Plot white noise 2 data
ts.plot(white_noise_2)
```

## 2.5 Estimate the white noise model
The WN model is an ARIMA(0, 0, 0) model. 

Fit a white noise (wN) model using the arima(..., order = c(0, 0, 0))

```{r}
# create a y
y <- arima.sim(model = list(order = c(0,0,0)), n = 100, mean = 95, sd = 10)
mean(y)
var(y)

# Fit the WN model to y using the arima command
arima(y, order = c(0, 0, 0))

```

From the comparisons, we can see that the *arima()* function estimates are very close to the sample mean and variance estiamtes. 


## 2.6 Simulate the random walk model

The random walk(RW) model is an **ARIMA(0, 1, 0)** model, in which middle entry of 1 indicates that the model's order of integration is 1. 

The *arima.sim()* function can simulate the RW by including model = list(order = c(0, 1, 0))

```{r}
# Generate a RW model using arima.sim
random_walk <- arima.sim(model = list(order = c(0, 1, 0)), n = 100)

# Plot random_walk
ts.plot(random_walk)

# Calculate the first difference series
random_walk_diff <- diff(x = random_walk, lag = 1)

# Plot random_walk_diff 
ts.plot(random_walk_diff)
```

The first difference of the *random_walk* daa is white noise data. The random walk is simply recursive white noise data. 


## 2.7 Simulate the random walk model with a drift

A random walk (RW) can have an upward or downward trajectory. This is done by including an intercept in the RW model . 

```{r}
# Generate a RW model with a drift using arima.sim
rw_drift <- arima.sim(model = list(order = c(0, 1, 0)), n = 100, mean = 1)

# Plot 
ts.plot(rw_drift)

# Calculate the first difference series
rw_drift_diff <- diff(rw_drift, lag = 1)

# Plot rw_drift_diff
ts.plot(rw_drift_diff)

```

Taking the first difference of the random walk data transformed it back into white noise data, regardless of the presence of the long-term drift. 


## 2.8  Estimate the random walk model 

For a given time series *y*, we can fit the random walk model with a drift by first difference the data, the fitting the white noise (WN) model to the differenced data using the arima() with order = c(0, 0, 0). 

```{r}
# generate a random walk time seris
rw_drift <- arima.sim(model = list(order = c(0, 1, 0)), n = 100, mean = 1)

#  Difference the random_walk data
rw_diff <- diff(rw_drift, lag = 1)

# Plot rw_diff
ts.plot(rw_diff)

# Now fit the WN model to the difference data
model_wn <- arima(x = rw_diff, order = c(0, 0, 0))
model_wn

# Store the value of the estimated time trend (intercept)
int_wn <- model_wn$coef

# Plot the original random_walk data
ts.plot(rw_drift)

# Use abline(0, ...) to add time trend to the figure
abline(a = 0, b = int_wn)

```

As shown in the figure, the arima() command correctly identified the time trend in the original random-walk data. 


## 2.9 Stationary 

Commonly departures from stationarity, including time trends, periodicity, and a lack of mean reversion. 

Are the white noise model or the random walk model stationary? 

The RW is always non-stationary, both with and without a drift term. 

* Start with a mean zero WN process and compute its running or cumulative sum, the result is a RW process. 

* Start with a mean no-zero WN process and compute its running or cumulative sum, the result is a RW process with a drift. 


```{r}
# Use arima.sim() to generate WN data
white_noise <- arima.sim(model = list(order = c(0, 0, 0)), n = 100)

# use cumsum() to convert the WN data to RW
random_walk <- cumsum(white_noise)

# Use arima.sim() to generate WN drift data
wn_drift <- arima.sim(model = list(order = c(0, 0, 0)), n = 100, mean = 0.4)

# Use cumsum() to convert the WN drift data to RW 
rw_drift <- cumsum(wn_drift)

# Plot all four data objects
plot.ts(cbind(white_noise, random_walk, wn_drift, rw_drift))

```


# 3. Correlation analysis and the autocorrelation function

"In this chapter, you will review the correlation coefficient, use it to compare two time series, and also apply it to compare a time series with its past, as an autocorrelation. You will discover the autocorrelation function (ACF) and practice estimating and visualizing autocorrelations for time series data."

## 3.1 Asset prices vs. asset returns
```{r}
# Plot eu_stocks
plot(eu_stocks)

# Convert prices to returns: ( a2 - a1)/a1 
returns <- (eu_stocks[-1, ] - eu_stocks[-nrow(eu_stocks), ] )/eu_stocks[-nrow(eu_stocks),]


# Convert return to ts
returns <- ts(returns, start = c(1991, 130), frequency = 260)

# Plot returns
plot(returns)


# Convert prices to log returns
logreturns <- diff(log(eu_stocks))

# Plot logreturns
plot(logreturns)

```

Daily net returns and daily log returns are two valuable metrics for financial data. 


## 3.2 Characteristics of financial time series

```{r}
# Convert prices to returns: ( a2 - a1)/a1 
returns <- (eu_stocks[-1, ] - eu_stocks[-nrow(eu_stocks), ] )/eu_stocks[-nrow(eu_stocks),] *100
# Convert return to ts
returns <- ts(returns, start = c(1991, 130), frequency = 260)

# Generate means 
colMeans(returns)

# Calculate sample variance 
returns %>% as.data.frame() %>%
  gather(key = "key", value = "value", DAX:FTSE) %>%
  group_by(key) %>%
  summarize(mean = mean(value), 
            var = var(value),
            sd = sd(value))

apply(returns, MARGIN = 2, FUN = var)

# Calculate sample standard deviation 
apply(returns, MARGIN = 2, FUN = sd)

# Display a histogram of percent returns for each index
par(mfrow = c(2, 2)) 
apply(returns, MARGIN = 2, FUN = hist, main = "", xlab = "Percentage Return")

# Display a histogram 
returns %>% as.data.frame() %>%
  gather(key = "key", value = "value", DAX:FTSE) %>% 
  ggplot(aes(x = value)) +
  geom_histogram() +
  facet_wrap(~key)

# Display a q-q plot 
returns %>% as.data.frame() %>%
  gather(key = "key", value = "value", DAX:FTSE) %>% 
  ggplot(aes(sample = value)) +
  stat_qq() +
  facet_wrap(~key)

```


## 3.3 Plotting pairs of data
```{r}
# Make a scatterplot of Germany major stock (DAX) and UK (FTSE )
plot(eu_stocks[ ,1], eu_stocks[, 2])


# Make a scatterplot matrix of eu_stocks
pairs(eu_stocks)

# Convert eu_stocks to log returns
logreturns <- diff(log(eu_stocks))
head(logreturns)
class(logreturns)

# Make a scatterplot matrix of logreturns
pairs(logreturns)

```

Pairs of data drawn from a multivariate normal distribution form a roughly elliptically shaped point cloud. 


## 3.4 Calculating sample covariances and correlations

```{r}
# Use covariance and variance of logreturn
cov(logreturns) 

# Correlation 
cor(logreturns)

```

The cor() and scatterplot matrix are intuitive outputs. 


## 3.5 Calculating Autocorrelations

Autocorrelations or lagged correlations are used to assess whether a time series is dependent on its past. 

```{r}
# Create x 
x <- eu_stocks[, 1] 
class(x)
n <- length(x)

# Define x_to  and x_t1
x_t1 <- x[-1]
x_t0 <-  x[-n]

# Plot x_to and x_t1
plot(x_t0, x_t1)

# correlation
cor(x_t0, x_t1)

# Use acf with x
acf(x, lag.max = 1, plot = FALSE)


```


## 3.7 Autocorrelation function

Autocorrelations can be estimated at many lags to better assess how a time series relats to its past. 
```{r}
# Generate ACF estiamtes for x up ot lag -10
acf(x, lag.max = 10, plot = FALSE)
```

## 3.8 Visulizing the autocorrelation function
```{r}
# x random number 
x <- rnorm(100)
acf(x)

# 
x <- 1:100 + rnorm(100, mean = 5, sd = 10)
acf(x)
```



# 4. Autoregression 

Learn autogressive (AR) model. Compare AR model with the random walk (RW) model. 

**The Autoregressive Model**

The Autoregressive (AR) recursion: 

$$Today = Constant + Slope * Yesterday + Noise$$


Mean centered version:

$$(Today - Mean) = Slope * (Yesterday - Mean) + Noise$$

More formally: 

$$Y_t-\mu = \phi(Y_{t-1} - \mu) + \epsilon_t$$
Where $\epsilon_t$ is mean zero white noise (WN). 


Three paramters: 

* The mean $\mu$

* The slop $\phi$

* The WN variance $\sigma^2_{\epsilon}$


* If slope $\phi = 0$ then: $Y_t = \mu + \epsilon_t$ and $Y_t$ is White Noise($\mu$, $\sigma^2_{\epsilon}$) 

* If slope $\phi \neq 0$ then $Y_t$ depends on both $\epsilon_t$ and $Y_{t-1}$. And the process $\{Y_t\}$ is autocorrelated. 

* Large values of $\phi$ lead to greater autocorrelation

* Nagative values of $\phi$ result in oscillatory time series. 


Random Walk: 

if $\mu = 0$ and slope $\phi = 1$, then: 
$$Y_t = Y_{t-1} + \epsilon_t$$


## 4.1 Simulate the autoregressive model 

The autoregressive (AR) model is arguably the most widely used time series model. It shares the very familiar interprepation of a simple linear regression, but here each observation is regressed on the previous observation. The AR model also includes the white noise (WN) and random walk (RW) models. 

Exercise: using arima.sim() to simulate and plot three different AR models with lsope paramters equal to 0.5, 0.9 and -0.75, respectively. 

```{r}
# Simulate an AR model with 0.5 slope
x <- arima.sim(model = list(ar = 0.5), n = 100)

# Simulate an AR model with 0.9 slope
y <- arima.sim(model = list(ar = 0.9), n = 100)


# Simulate an AR model with -0.75 slope
z <- arima.sim(model = list(ar = -0.75), n = 100)

# Plot the simulated data of z, y and z
plot.ts(cbind(x, y, z))

```

As shown in the plot, x data show a just a moderate amount of autocorrelation, while y data show a larger amount of autocorrelation. Alternatively, z data tends to oscillate considerably from one observation to the next. 


## 4.2 Estimate the autocorrelation function (ACF)

```{r}
# Calculate the ACF for x
acf(x)

# Calculate the ACF for y
acf(y)

# Calculate the ACF for z
acf(z)
```

## 4.3 compare the random walk (RW) and autoregression (AR) models

The random walk (RW) model is a special case of the autoregressive (AR) model, in which the slope paramter equals to 1. The RW model is not stationary and exhibits very strong persistance. Its sample autocovariance function (ACF) also decays to zero very slowly, meaning past values have a long lasting impact on current values. 

```{r}
# Simulate and plot AR model with slope 0.9
x <- arima.sim(model = list(ar = 0.9), n = 200)
ts.plot(x)
acf(x)

# Simulate and plot AR model with slope 0.98
y <- arima.sim(model = list(ar = 0.98), n = 200)
ts.plot(y)
acf(y)

# Simulate and plot RW model
z <- arima.sim(model = list(order = c(0, 1, 0)), n = 200)
ts.plot(z)
acf(z)

```


## 4.4 AR Model Estimation and Forecasting

* One-month US infaltion rate (in percent, annual rate)

* Monthly observatons from 1950 through 1990

```{r}
data(Mishkin, package = "Ecdat")
summary(Mishkin)

# creat time series 
inflation <- as.ts(Mishkin[, 1])

# plot
ts.plot(inflation)

acf(inflation)
```


**AR Model: Inflation Rate**

$$(Today - Mean) = Slope * (Yesterday - Mean) + Noise$$

$$Y_t - \mu = \phi(Y_{t-1} - \mu) + \epsilon_t$$

$$\epsilon_t \sim {\sf White Noise(0, \sigma^2_{\epsilon})}  $$


```{r}
AR_inflation <- arima(inflation, order = c(1, 0, 0))
AR_inflation
```

$ar1 = \hat{\phi}$, $intercpt = \hat{\mu}$, $sigma^2 = \hat{\sigma}^2_{\epsilon}$

* AR fitted vlaues: $\hat{Today} = \hat{Mean} + \hat{Slope} * (Yesterday - \hat{Mean})$

$$\hat{Y_t} = \hat{\mu} + \hat{\phi} (Y_{t-1} - \hat{\mu})$$

* $Residuals = Today - \hat{Today}$

$$\hat{\epsilon_t} = Y_t - \hat{Y_t$$

```{r}
## add fitted values
AR_inflation_fitted <- inflation - residuals(AR_inflation)

ts.plot(inflation)
points(AR_inflation_fitted, type = "l", col = "red", lty = 2)
```


**Forecasting**

```{r}
predict(AR_inflation)
predict(AR_inflation, n.ahead = 6)
```


## 4.5 Estimate the autoregression (AR) model (exercise) 

```{r}
# Fit the AR model to AirPassengers

AR <- arima(AirPassengers, order = c(1, 0, 0))
print(AR)

## plot the series and fitted values 
AR_fitted <- AirPassengers - residuals(AR)
ts.plot(AirPassengers)
points(AR_fitted, type = "l", col = 2, lty = 2)

```

## 4.6 Simple forecasts from an estimated AR model

```{r}
# Fit an AR model to Nile
ts.plot(Nile)
acf(Nile)

AR_fit <- arima(Nile, order = c(1, 0, 0))
AR_fit

# Use predict() to make a 1-step forecast
predict_AR <- predict(AR_fit)

# Obtain the 1-step forecast using $pred[1]
predict_AR$pred[1]

# Use predict to make 1-step through 10-step forecasts
predict(AR_fit, n.ahead = 10)

# Plot the Nile series plus the forecast and 95% prediction intervals
ts.plot(Nile, xlim = c(1871, 1980))
AR_forecast <- predict(AR_fit, n.ahead = 10)$pred
AR_forecast_se <- predict(AR_fit, n.ahead = 10)$se
points(AR_forecast, type = "l", col = 2)
points(AR_forecast - 2 * AR_forecast_se, type = "l", col = 2, lty = 2)
points(AR_forecast + 2 * AR_forecast_se, type = "l", col = 2, lty = 2)

# add fitted 
AR_fitted <- Nile - residuals(AR_fit)
points(AR_fitted, type = "l", col = 2, lty = 2)
```


