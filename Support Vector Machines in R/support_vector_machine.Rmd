---
title: "Supper Vector Machine in R"
author: "Jeff Li"
date: "`r Sys.Date()`"
output: 
  html_document: 
    toc: true
    toc_depth: 3
    toc_float: true
---

"I maintained my edge by always being a student; you will always have something new to learn". - Jackie Joyner Kersee


Ref: Awati, K., Support Vector Machines in R. https://www.datacamp.com/courses/support-vector-machines-in-r, 2018.


**Course Description**

This course develops data scientists' sunderstanding of the SVM in R. It covers hard/soft margins, kernel trick, different types of kernels, and how to tune SVM parameters. 



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## (I) Load Required Libraries
```{r, message = FALSE}
library(tidyverse)
library(purrr)
library(broom)
library(rsample)
library(Metrics)
library(ranger)

library(e1071)

```


# 1. Introduction

Objectives: 

* introduce key concepts of SVM, understand how SVMs work, learn options available in the algorithm and situation that SVM can do good job

* Create a linearly separable dataset


## 1.1 Taste of soft drink 

**Maximum margin separator**: the best decision boundary 

```{r}

# (a) read data
soft_drink_sugar <- read.table("data/soft_drink_sugar.txt") %>%
  as_tibble()

head(soft_drink_sugar)
class(soft_drink_sugar)


#print variable names
df <- soft_drink_sugar
names(df)

#build plot
plot_df <- ggplot(data = df, aes(x = sugar_content, y = c(0))) + 
    geom_point() + 
    geom_text(label = df$sugar_content, size = 2.5, vjust = 2, hjust = 0.5)

#display plot
plot_df

```

As shown in the figure, this dataset is separable by a **decision boundary**, and the classes do not overlap.  The **maximal margin separator** is the dicision boundary that is furthest from both classes. It is located halfway between the extreme points in each class. 

```{r}
#The maximal margin separator is at the midpoint of the two extreme points in each cluster.
mm_separator <- (8.9 + 10)/2


#create data frame
separator <- data.frame(sep = c(mm_separator))

#add ggplot layer 
plot_sep <- plot_df + geom_point(data = separator, aes(x = sep, y = c(0)), color = "blue", size = 4)

#display plot
plot_sep

```

It should be clear form the plot that the blue point is the best possible separator. 


## 1.2 Generate a 2d uniformly distributed dataset

The specific objective is to create a dataset that will be used to illustrate the basic principles of support vector machines.  a 2 dimensional uniformly distributed dataset containing 600 data points. 



```{r}
#set seed to ensure reproducibility 
set.seed(42)

#set number of data points. 
n <- 600

#Generate data frame with two uniformly distributed predictors lying between 0 and 1.
df <- tibble(x1 = runif(n), 
             x2 = runif(n))

# classify data points depending on location (x2 = 0 + 1.4 * x1, decision boundary) 
df <- df %>%
  mutate(y = factor( ifelse(x2 - 1.4*x1 < 0, -1, 1), levels = c(-1, 1))) 
 
str(df)

#----------introduce a margine of 0.07----------
#set margin
delta <- 0.07

# retain only those points that lie outside the margin
df1 <- df %>%
  mutate(abs_x_to_db = abs(1.4*x1 - x2)) %>%
  filter(abs_x_to_db >= 0.07) %>%
  select(-abs_x_to_db)

#build plot
plot_margins <- ggplot(data = df1, aes(x = x1, y = x2, color = y)) + 
  geom_point() + 
  scale_color_manual(values = c("red", "blue")) + 
  geom_abline(slope = 1.4, intercept = 0) +
  geom_abline(slope = 1.4, intercept = 0.07, linetype = "dashed") +
  geom_abline(slope = 1.4, intercept = -0.07, linetype = "dashed")
 
#display plot 
plot_margins
         
```



# 2. Support Vector Classification - Linear Kernels

**Objectives**: 

* learn how to apply svm algorithm to a linearly separable dataset

* learn how to use ggplot to visulize results

* learn how to deal with multiclass problems


## 2.1 Build a linear SVM classifier

Using the R package e1071 to build a svm classifier. Why is the R package e1071 named so? Authors' statistic department has code: e107 and e1071 belongs to computational intelligence within that department. 


```{r}
#split train and test data in an 80/20 proportion

df <- df %>% 
  mutate(train = ifelse(runif(nrow(df)) < 0.8, 1, 0))

#assign training rows to data frame trainset
trainset <- df %>%
  filter(train == 1) %>%
  select(-train)

#assign test rows to data frame testset
testset <- df %>% 
  filter(train == 0) %>%
  select(-train)


#build svm model, setting required parameters
svm_model<- e1071::svm(y ~ ., 
                data = trainset, 
                type = "C-classification", 
                kernel = "linear", 
                scale = FALSE)

svm_model

#list components of model
names(svm_model)


#list values of the SV, index and rho
message("SV: support vector")
head(svm_model$SV, n = 5) 

message("index: index of support vectors in training dataset" )
head(svm_model$index, n = 5)
length(svm_model$index)

message("rho: negative intercept (unweighted)")
svm_model$rho

message("weighting coefficients for support vectors")
head(svm_model$coefs, n = 5)

#compute training accuracy
pred_train <- predict(svm_model, trainset)
mean(pred_train == trainset$y)

#compute test accuracy
pred_test <- predict(svm_model, testset)

mean(pred_test == testset$y)
```

## 2.2 Visualizing Linear SVMs

```{r}

#build scatter plot of training dataset
scatter_plot <- trainset %>%
  ggplot(aes(x = x1, y = x2, color = y)) + 
  geom_point(alpha = 0.7) + 
  scale_color_manual(values = c("red", "blue"))
 
#add plot layer marking out the support vectors 
layered_plot <- 
  scatter_plot + 
  geom_point(data = trainset[svm_model$index, ], aes(x = x1, y = x2), color = "purple", size = 4, alpha = 0.4)

#display plot
layered_plot


#calculate slope and intercept of decision boundary from weight vector and svm model

# build the weight vector, w, from coefs and SV elements
w <- t(svm_model$coefs) %*% svm_model$SV

# calcualate slope 
slope_1 <- -w[1]/w[2]

# calculate intercept 
intercept_1 <- svm_model$rho/w[2]

#add decision boundary
plot_decision <- layered_plot + geom_abline(slope = slope_1, intercept = intercept_1) 

#add margin boundaries
plot_margins <- plot_decision + 
 geom_abline(slope = slope_1, intercept = intercept_1 - 1/w[2], linetype = "dashed")+
 geom_abline(slope = slope_1, intercept = intercept_1 + 1/w[2], linetype = "dashed")

#display plot
plot_margins
```

We can visulaize the decision regions and support vectors using the svm plot function. 

**Soft margin classifiers**: allow for uncertainty in location/shape of boundary: never perfectly linear; usually unknow. 

We can also visualize the dicision boundary using the svm plot() funtion: 
```{r}
plot(x = svm_model, data = trainset)
```



## 2.3 Tuning a linear SVM

```{r}
#build svm model, cost = 1
svm_model_1 <- svm(y ~ .,
                   data = trainset,
                   type = "C-classification",
                   cost = 1,
                   kernel = "linear",
                   scale = FALSE)

#print model details
svm_model_1


#build svm model, cost = 100
svm_model_100 <- svm(y ~ .,
                   data = trainset,
                   type = "C-classification",
                   cost = 100,
                   kernel = "linear",
                   scale = FALSE)

#print model details
svm_model_100

tibble(num_of_support_vector_cost_1 = length(svm_model_1$index),
       num_of_support_vector_cost_100 = length(svm_model_100$index))



       
```

As the cost increases, the margin becomes narrower, adn the number of support vectors decreases, 


## 2.4 Compare decision boundaries and margins


```{r}

#calculate slope and intercept of decision boundary from weight vector and svm model

# build the weight vector, w, from coefs and SV elements
w_100 <- t(svm_model_100$coefs) %*% svm_model_100$SV

# calcualate slope 
slope_100 <- -w_100[1]/w_100[2]

# calculate intercept 
intercept_100 <- svm_model_100$rho/w_100[2]


#add margin boundaries using cost = 100 
plot_margins_2 <- plot_margins + 
 geom_abline(slope = slope_100, intercept = intercept_100 - 1/w_100[2], linetype = "dashed", color = "green") +
 geom_abline(slope = slope_100, intercept = intercept_100 + 1/w_100[2], linetype = "dashed", color = "green")

#display plot
plot_margins_2

```


Compare test set accuracy: cost = 1 vs cost = 100

```{r}
# compare test accuracy
pred_test_1 <- predict(svm_model_1, testset)
pred_test_100 <- predict(svm_model_100, testset)

tibble(svm_model_1_accuracy = mean(pred_test_1 == testset$y), 
       svm_model_100_accuracy = mean(pred_test_100 == testset$y)) 
```

Low cost, wide margin (a soft margin linear SVM) vs High cost, narrow margin (a hard margin linear SVM): slightly better accuracy (0.984 vs 0.976). 

Linear soft margin classifiers are most likely to be useful when Working with a dataset that is almost linearly separable.  In other words, a soft margin linear classifier would work well for a nearly separable datase. 


## 2.5 Multclass SVM classifier

```{r}
# (a) Generate trainset and testset

n <- 645
trainset <- tibble(x1 = runif(n), x2 = runif(n)) %>%
  mutate(y = factor( case_when( x2 <= 1/2*x1 ~ -1, 
                        x2 >=  0.866*x1 ~ 1, 
                        TRUE ~ 0) )) 
  
n2 <- 155
testset <- tibble(x1 = runif(n2), x2 = runif(n2)) %>%
  mutate(y = factor( case_when( x2 <= 1/2*x1 ~ -1, 
                                x2 >=  0.866*x1 ~ 1, 
                                TRUE ~ 0 ))) 

# chack 3-class distributions between train and test
list( table(trainset$y)/nrow(trainset), 
      table(testset$y)/nrow(testset))

# plot 
trainset %>%
  ggplot(aes(x = x1, y = x2, color = y)) + 
  geom_point(alpha = 0.6)

testset %>%
  ggplot(aes(x = x1, y = x2, color = y)) + 
  geom_point(alpha = 0.6)

# (b) build svm model:
svm_model <- svm(y ~ ., 
                 data = trainset, 
                 type = "C-classification", 
                 kernel = "linear", 
                 scale = FALSE, 
                 cost = 1)
svm_model

# compute training and test accuracy
pred_train <- predict(svm_model, trainset)
pred_test <- predict(svm_model, testset)

# confusion table 
conf_train <- table(trainset$y, pred_train)
conf_train  

conf_test <- table(testset$y, pred_test)
conf_test

# training  and test accuracy 
sum(diag(conf_train))/sum(conf_train)

sum(diag(conf_test))/sum(conf_test)

# plot for training set 

plot(svm_model, trainset)

```


## 2.6 Accuracy for n distinct 80/20 train/test partitions 

```{r}
# (a) data 
head(iris)
dim(iris)
summary(iris)

# calculate accuracy for n distinc 80/20 train/test partions
#calculate accuracy for n distinct 80/20 train/test partitions
accuracy <- rep(0, 100)

for (i in 1:100) { 
    iris[, "train"] <- ifelse(runif(nrow(iris)) < 0.8, 1, 0)
    trainColNum <- grep("train", names(iris))
    
    trainset <- iris[iris$train == 1, -trainColNum]
    testset <- iris[iris$train == 0, -trainColNum]
    
    svm_model <- svm(Species~ ., 
                     data = trainset, 
                     type = "C-classification", 
                     kernel = "linear")
    
    pred_test <- predict(svm_model, testset)
    
    accuracy[i] <- mean(pred_test == testset$Species)
}

#mean and standard deviation of accuracy
tibble(n = length(accuracy), 
       accuracy_mean = mean(accuracy), 
       accuracy_sd  = sd(accuracy))

```

The high accuracy and low sd confirms that the dataset is almont linearly separable. 



# 3. Polynomial Kernels

Objectives: 

* Develop skills to use polynomial kernels for circular decision boundary


## 3.1 Generating a radially separable dataset
```{r}
# (a) #Generate data frame with two uniformly distributed predictors, x1 and x2
n <- 400
set.seed(1)

radium <- 0.8

df <- tibble(x1 = runif(n, min = -1, max = 1), 
             x2 = runif(n, min = -1, max = 1)) %>%
  mutate( y = factor( ifelse( ((x1 - mean(x1))^2 + (x2 - mean(x2))^2) <= radium^2, 1, -1), levels = c(-1, 1))) 

table(df$y)
             
# (b) visualizing the dataset
scatter_plot <- df %>%
  ggplot(aes(x = x1, y = x2, color = y)) + 
  geom_point() + 
  scale_color_manual(values = c("-1" = "red", "1" = "blue"))

scatter_plot

# (b) define a circle data

circle_fn <- function(center_x1, center_x2, radium) {
  angle = seq(from = 0, to = 2*pi, length.out = 100)
  tibble(x1 = center_x1 + radium * cos(angle), 
         x2 =center_x2 + radium * sin(angle))
}

boundary_data <- circle_fn(center_x1 = mean(df$x1), center_x2 = mean(df$x2), radium) 

scatter_plot + geom_path(data = boundary_data,  aes(x = x1, y = x2), inherit.aes = FALSE)

```

## 3.2 Accuracy of Linear SVM

calculate the average accuracy for a default cost linear SVM using 100 different training/test partitions of the dataset you generated in the first lesson of this chapter. 

```{r}
# Print average accuracy and standard deviation
accuracy <- rep(NA, 100)
set.seed(2)

# Calculate accuracies for 100 training/test partitions
for (i in 1:100){
    df[, "train"] <- ifelse(runif(nrow(df)) < 0.8, 1, 0)
    trainset <- df[df$train == 1, ]
    testset <- df[df$train == 0, ]
    trainColNum <- grep("train", names(trainset))
    trainset <- trainset[, -trainColNum]
    testset <- testset[, -trainColNum]
    svm_model <- svm(y ~ ., data = trainset, type = "C-classification", kernel = "linear")
    pred_test <- predict(svm_model, testset)
    accuracy[i] <- mean(pred_test == testset$y)
}

# Print average accuracy and standard deviation
mean(accuracy)
sd(accuracy)
```

Using *kernel = "linear"*, the model is not better than flipping a coin. Because of the data are not linear seperable, they are created by a circle boundary. 




# 4. Radial Basis Function Kernels 

Objectives: 

* Learn how to use Radial Basis Function (RBF) kernel for complex dataset

```{r}

```

