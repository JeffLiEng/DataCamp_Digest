ls(package:sparklyr", patter = "^ml")
ls("package:sparklyr", patter = "^ml")
library(sparklyr)
ls(pattern = "^ml")
ls("package:sparklyr", pattern = "^ml")
knitr::opts_chunk$set(echo = TRUE)
ml_naive_bayes
library(sparklyr)
timbre
load("data/timbre.rds"
load("data/timbre.rds")
timbre
readRDS("data/timbre.rds")
timbre <- readRDS("data/timbre.rds")
head(timbre)
# mean of coloumns
mean_timbre <- colMeans(timbre)
mean_bimbre
mean_timbre
# (b) list the files in the parquet dir
filenames <- dir(parquet_dir, full.names = TRUE)
# (a) Define a parquet directory
parquet_dir <- "data/timbre_parquet"
# (b) list the files in the parquet dir
filenames <- dir(parquet_dir, full.names = TRUE)
filenames
# (c) Show the filenames and their sizes
data_frame(filename = basename(filenames),
size_bytes = file.size(filenames))
# (c) Show the filenames and their sizes
data.frame(filename = basename(filenames),
size_bytes = file.size(filenames))
library(tidyverse)
# (c) Show the filenames and their sizes
data_frame(filename = basename(filenames),
size_bytes = file.size(filenames))
# (d) Import the data into Spark
timbre_tbl <- spark_read_parquet(spark_conn, "timbre", parquet_dir)
# (a) Install Spark on my local system
spark_install()
# Connect to your Spark cluster
spark_conn <- spark_connect(master = "local")
# Disconnect from Spark
spark_disconnect(sc = spark_conn)
# Connect to your Spark cluster
spark_conn <- spark_connect(master = "local")
# Connect to your Spark cluster
spark_conn <- spark_connect(master = "local")
# Print the version of Spark
spark_version(sc = spark_conn)
# Disconnect from Spark
spark_disconnect(sc = spark_conn)
library(dplyr)
# Explore track_metadata structure
str(track_metadata)
# Explore track_metadata structure
track_metadata <- readRDS("data/track_metadata.rds")
str(track_metadata)
spark_conn <- spark_connect("local")
# compy track_metadata to Spark
track_metadata_tbl <- copy_to(dest = spark_conn, df = track_metadata)
# Connect to your Spark cluster
spark_conn <- spark_connect("local")
# Copy track_metadata to Spark
track_metadata_tbl <- copy_to(dest = spark_conn, df = track_metadata)
# Copy track_metadata to Spark
track_metadata_tbl <- copy_to(dest = spark_conn, df = track_metadata, overwrite = TRUE)
spark_conn
Sys.getenv()
Sys.getenv("SPARK_HOME")
Sys.getenv(SPARK_HOME = "/home/spark")
# Connect to your Spark cluster
spark_conn <- spark_connect(master = "local")
# Print the version of Spark
spark_version(sc = spark_conn)
# Disconnect from Spark
spark_disconnect(sc = spark_conn)
# Connect to your Spark cluster
spark_conn <- spark_connect(master = "local")
# Print the version of Spark
spark_version(sc = spark_conn)
# Disconnect from Spark
spark_disconnect(sc = spark_conn)
# Connect to your Spark cluster
spark_conn <- spark_connect(master = "local")
ls("package:sparklyr", pattern = "^ft") %>%
head()
head(mtcars)
mtcars_sparklyr <- copy_to(spark_conn, mtcars)
mtcars_sparklyr
# Explore track_metadata structure
track_metadata <- readRDS("data/track_metadata.rds")
str(track_metadata)
# Connect to your Spark cluster
spark_conn <- spark_connect("local")
# Copy track_metadata to Spark
track_metadata_tbl <- copy_to(dest = spark_conn, df = track_metadata, overwrite = TRUE)
# Copy track_metadata to Spark
track_metadata_tbl <- copy_to(dest = spark_conn, track_metadata, overwrite = TRUE)
# Copy track_metadata to Spark
track_metadata_tbl <- copy_to(spark_conn, track_metadata, overwrite = TRUE)
class(track_metadata)
dim(track_metadata)
# List the data frames available in Spark
src_tbls(spark_conn)
track_metadata_tbl
mtcars_sparklyr
mtcars_sparklyr <- copy_to(spark_conn, mtcars)
mtcars_sparklyr <- copy_to(spark_conn, mtcars, overwrite = TRUE)
mtcars_sparklyr
# List the data frames available in Spark
src_tbls(spark_conn)
mtcars_sparklyr
track_metadata_tbl
track_metadata
# Disconnect from Spark
spark_disconnect(spark_conn)
# Explore track_metadata structure
track_metadata <- readRDS("data/track_metadata.rds")
# Connect to your Spark cluster
spark_conn <- spark_connect("local")
# Connect to your Spark cluster
spark_conn <- spark_connect("local")
# List the data frames available in Spark
src_tbls(spark_conn)
# Copy track_metadata to Spark
track_metadata_tbl <- copy_to(spark_conn, track_metadata, overwrite = TRUE)
track_metadata_tbl
mtcars_sparklyr <- copy_to(spark_conn, mtcars, overwrite = TRUE)
mtcars_sparklyr
# List the data frames available in Spark
src_tbls(spark_conn)
# Disconnect from Spark
spark_disconnect(spark_conn)
# Split data
mtcars_sparklyr_part <- mtcars_sparklyr %>%
sdf_partition(train = 0.8, test = 0.2)
# Connect to your Spark cluster
spark_conn <- spark_connect("local")
# Connect to your Spark cluster
spark_conn <- spark_connect("local")
# Copy track_metadata to Spark
track_metadata_tbl <- copy_to(spark_conn, track_metadata, overwrite = TRUE)
mtcars_sparklyr <- copy_to(spark_conn, mtcars, overwrite = TRUE)
mtcars_sparklyr
# Split data
mtcars_sparklyr_part <- mtcars_sparklyr %>%
sdf_partition(train = 0.8, test = 0.2)
# Split data
mtcars_sparklyr_part <- mtcars_sparklyr %>%
sdf_partition(train = 0.8, test = 0.2)
mtcars_sparklyr_part
fit_random_forest <- ml_random_forest(
mtcars_sparklyr_part$train, # the training partion
response = "am",
features = features, # the names minus the outcome
col.sample.rate = 0.25,
impurity = "entropy",
max.bins = 32L,
max.depth = 5L,
num.trees = 100L,
type = "classification"
)
features <- colnames(mtcars_sparklyr)[colnames(mtcars_sparklyr) != "am"]
fit_random_forest <- ml_random_forest(
mtcars_sparklyr_part$train, # the training partion
response = "am",
features = features, # the names minus the outcome
col.sample.rate = 0.25,
impurity = "entropy",
max.bins = 32L,
max.depth = 5L,
num.trees = 100L,
type = "classification"
)
ml_tree_feature_importance(sc, fit_random_forest)
ml_tree_feature_importance(spark_conn, fit_random_forest)
summary(fit_random_forest)
ml_tree_feature_importance(spark_conn, fit_random_forest)
# Connect to your Spark cluster
spark_conn <- spark_connect(master = "local")
# Print the version of Spark
spark_version(sc = spark_conn)
ls("package:sparklyr", pattern = "^ft") %>%
head()
mtcars_sparklyr <- copy_to(spark_conn, mtcars, overwrite = TRUE)
mtcars_sparklyr
# Split data
mtcars_sparklyr_part <- mtcars_sparklyr %>%
sdf_partition(train = 0.8, test = 0.2)
features <- colnames(mtcars_sparklyr)[colnames(mtcars_sparklyr) != "am"]
fit_random_forest <- ml_random_forest(
mtcars_sparklyr_part$train, # the training partion
response = "am",
features = features, # the names minus the outcome
col.sample.rate = 0.25,
impurity = "entropy",
max.bins = 32L,
max.depth = 5L,
num.trees = 100L,
type = "classification"
)
summary(fit_random_forest)
ml_tree_feature_importance(spark_conn, fit_random_forest)
sparklyr::ml_tree_feature_importance(spark_conn, fit_random_forest)
ls("package:sparklyr", pattern = "^ml") %>%
head()
ml_classification_eval(fit_random_forest)
ml_classification_eval(spark_conn, fit_random_forest)
sparklyr::ml_classification_eval(spark_conn, fit_random_forest)
# Disconnect from Spark
spark_disconnect(sc = spark_conn)
curve(y ~ x^2)
?curve
curve(y ~ x^2, -10, 10)
curve(x^2, -10, 10)
curve(2*x^4 - 3*2^(1/3) * x^2 - 16*x + 1, -10, 10)
curve(**x^3 - 6 *2^(1/3) * x - 16)
curve(8*x^3 - 6 *2^(1/3) * x - 16)
curve(8*x^3 - 6 *2^(1/3) * x - 16, -10, 10)
curve(2*x^4 - 3*2^(1/3) * x^2 - 16*x + 1, -10, 10)
curve(8*x^3 - 6 *2^(1/3) * x - 16, -10, 10)
curve(16*x^2 - 6 * 2^(1/3), -10, 10)
curve(2*x^4 -12 * x^2 - 16*x + 1, -10, 10)
curve(8*x^3 -24 * x - 16, -10, 10)
curve(24*x^2 - 24, -10, 10)
# Connect to your Spark cluster
spark_conn <- spark_connect("local")
# Connect to your Spark cluster
spark_conn <- spark_connect("local")
# copy mtcars to spark
mtcars_sparklyr <- copy_to(spark_conn, mtcars, overwrite = TRUE)
mtcars_sparklyr
# List the data frames available in Spark
src_tbls(spark_conn)
mtcars_sparklyr
str(mtcars_sparklyr)
track_mtcars_tbl <- tbl(spark_conn, "mtcars_sparklyr")
# Connect to your Spark cluster
spark_conn <- spark_connect("local")
# copy mtcars to spark
mtcars_sparklyr <- copy_to(spark_conn, mtcars, overwrite = TRUE)
mtcars_sparklyr
track_mtcars_tbl <- tbl(spark_conn, "mtcars_sparklyr")
?tbl
# List the data frames available in Spark
src_tbls(spark_conn)
track_mtcars_tbl <- tbl(spark_conn, "mtcars")
# Link the data in Spark
track_mtcars_tbl <- tbl(spark_conn, "mtcars")
# See how big the dataset is
dim(track_mtcars_tbl)
# See the tibble size
object_size(track_mtcars_tbl)
# Link the data in Spark
track_mtcars_tbl <- tbl(spark_conn, "mtcars")
# See how big the dataset is
dim(track_mtcars_tbl)
# See the tibble size
object_size(track_mtcars_tbl)
# See the tibble size
pryr::object_size(track_mtcars_tbl)
curve(2*x^4 -12 * x^2 - 16*x + 1, -10, 10)
curve(8*x^3 -24 * x - 16, -10, 10)
# Explore track_metadata structure
track_metadata <- readRDS("data/track_metadata.rds")
str(track_metadata)
class(track_metadata)
dim(track_metadata)
track_metadata
track_metadata_tbl <- copy_to(spark_conn, track_metadata, overwrite = TRUE)
track_metadata_tbl
str(mtcars_sparklyr)
# print 5 rows, all columns
print(track_mtcars_tbl, n = 5, width = Inf)
