---
title: "Linear Algebra For Data Science in R"
author: "Jeff Li"
date: "`r Sys.Date()`"
output: 
  html_document: 
    toc: true
    toc_depth: 3
    toc_float: true
---


__Course Description__

Linear algebra is one of the most important set of tools in applied mathematics and data science. In this course, you’ll learn how to work with vectors and matrices, solve matrix-vector equations, perform eigenvalue/eigenvector analyses and use principal component analysis to do dimension reduction on real-world datasets. All analyses will be performed in R, one of the world’s most-popular programming languages.


Reference: Eager, E. Linear Algebra For Data Science in R, www.datacamp.com, Paid class: 10/18/2018 - 10/20/2018. 

Note: Some course materials have been modified to run locally and train students by Jeff Li. 


# (I) Load required libraries

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
```

# 1. Introduction to Linear Algebra

## 1.1 Create vector in R 
```{r}
# Creating three 3's and four 4's, respectively
rep(3, 3)
rep(4, 4)

# Creating a vector with the first three even numbers and the first three odd numbers
seq(2, 6, by = 2)
seq(1, 5, by = 2)

# Re-creating the previous four vectors using the 'c' command
c(3, 3, 3)
c(4, 4, 4, 4)

c(2, 4, 6)
c(1, 3, 5)
```

In **R**, We can create vectors using some simple commands. 


## 1.2 The Algebra of Vectors

```{r}
# create the vectors x, y, and z
(x <- 1:7)
(y <- seq(2, 14, by = 2))
(z <- c(1, 1, 2))

# Add x to y and print

print(x + y)

# Multiply z by 2 and print
print(2 * z)

# Multiply x and y by each other and print
print(x * y)

# Add x to z, if possible, and print
print(x + z)

```

## 1.3 Creating Matrices in R
```{r}
# Create a matrix of all 1's and all 2's that are 2 by 3 and 3 by 2, respectively
matrix(1, 2, 3)

print(matrix(2, 3, 2))

# Create a matrix and changing the byrow designation.
matrix(c(1, 2, 3, 2), nrow = 2, ncol = 2, byrow = FALSE)
matrix(c(1, 2, 3, 2), nrow = 2, ncol = 2, byrow = TRUE)

# Add A to the previously-created matrix
A <- matrix(1, 2, 2)
A + matrix(c(1, 2, 3, 2), nrow = 2, ncol = 2, byrow = TRUE)
```


## 1.4 Matrix Multiplication as a Transformation

* Stretch 

* Shrink

* Reflect 

The following examples show how multiplication by a matrix can alter a vector. 

```{r}
# (a) Stretch: Need to stretch the x (first) componenet of a vector by factor of 4: c(1, 1)
A <- matrix(c(4, 0, 0, 1), nrow = 2, ncol = 2)
A

A %*% c(1, 1)

# (b) Shrinks the y (second) componenet of a vector b <- c(1, 1) by 50%
B <- matrix(c(1, 0, 0, 0.5), nrow = 2, ncol =2 )

B %*% c(1, 1)

# (c) reflect a vector about the y-axis
A <- matrix(c(-1, 0, 0, 1), nrow = 2, ncol = 2)
A

A %*% c(2, 2)

# (d) reflect a vector about the x-axis
B <- matrix(c(1, 0, 0, -1), nrow = 2, ncol =2)
B %*% c(2, 2)

# (e) rotation

C <- matrix(c(-1, 0, 0, -1), nrow = 2, ncol =2 )
C %*% c(2, 2)

```

## 1.5 Matrix Multiplication 

* Matrix multiplication is a complex transformation. 

* Matrix multiplication - Order matters

* matrix inverse (x * 1/x = 1, A %*% Ainv = I)

```{r}
# Take the inverse of the 2 by 2 identity matrix
solve(diag(2))

# Take the inverse of the matrix A
A <- matrix(c(1, -1, 2, 2), nrow = 2, ncol = 2, byrow = FALSE)
A
Ainv <- solve(A)

# Multiply A by its inverse on the left
Ainv %*% A

# Multiply A by its inverse on the right
A %*% Ainv

```


# 2. Matrix-Vector Equations

Many machine learning algorithms boil down to solving a matrix-vector equation. In this chapter, you learn what matrix-vector equations are trying to accomplish and how to solve them in R.

## 2.1 The Meaning of Ax = b and Exploring WNBA data

A great deal of applied mathematics and statistics, as well as data science, ends in a matrix-vector equation of the form: 

$$A*\bar{X} = \bar{b}$$

In this chapter, we will work with a matrix-vector model for team strength in the Women's Nation Basketball Association (WNBA) at the conclusion of the 2017 season. These team strengths cab be used to predict who will win a match between any two teams. 

The WNBA has 12 teams, so Messey Matrix M will be 12 x 12. 

```{r}
# Read the Massey Matrix M 
M <- read_csv("data/WNBA_Data_2017_M.csv")
rownames(M) <-  colnames(M)


# Read the vector of point differentials f 
f <- read_csv("data/WNBA_Data_2017_f.csv")
rownames(f) <- rownames(M)
f

# Find the sum of the first column
sum(M[, 1])

# Find the sum of the vector 
sum(f)

```

## 2.2 Matrix Inversibility and Adjusting the Messey Matrix 

If a matrix is not (computationally) invertible, then an adjustment needs to be made. 

M: already add a row of 1's on the bottom of the matrix M, column of -1's to the far right of M, and a 0 to the bottom of the vector of point differentials f .

## 2.3 2017 WNBA Ratings
```{r}
# Solve for r and rename column
M <- as.matrix(M)
f <- as.matrix(f)
r <- solve(M) %*% f

colnames(r) <- "Rating"

# print r
print(r)


# Find the rating vector using ginv
r <- MASS::ginv(M)%*%f
colnames(r) <- "Rating"
print(r)

```


# 3. Eigenvalues and Eigenvectors

Matrix operations are complex. Eigenvalue/eigenvector analyses allow you to decompose these operations into simpler ones for the sake of image recognition, genomic analysis, and more!

## 3.1 Scaling Different Axes

transform a two-dimensional vector so that the first element doubled in size, while the second element was cut by a third.

```{r}
# create the matxix A
A <- matrix(c(2, 0, 0, 0.66667), nrow = 2)
A

A %*% c(1, 1)
```
Multiplied by a matrix that contracts and stretches elements of a vector!


## 3.2 Find Eigenvalues in R

```{r}
# build A
A <- matrix(c(-1, 0, 0, 2, 7, 0, 4, 12, -4), nrow = 3)
A

# Show that 7 is an eigenvalue for A
x1 <- c(0.2425356, 0.9701425, 0)

A %*% x1 - 7 * x1

# Show that -4 is an eigenvalue for A
x2 <- c(-0.3789810, -0.6821657, 0.6253186)

A %*% x2 - (-4) * x2

# Show that -1 is an eigenvalue for A
x3 <- c(1, 0, 0)
A %*% x3 - (-1)*x3

# show that a scalar multipleof an eigenvector is still an eigenvector of a matrix
# Show that double an eigenvector is still an eigenvector
A%*%((2)*c(0.2425356, 0.9701425, 0)) - 7*(2)*c(0.2425356, 0.9701425, 0)

# Show half of an eigenvector is still an eigenvector
A%*%((0.5)*c(0.2425356, 0.9701425, 0)) - 7*(0.5)*c(0.2425356, 0.9701425, 0)
```

## 3.3 Verifying the Math on Eigenvalues

Find the eigenvalues of a matrix, and show that they satisfy the properties discussed in the lecture.

```{r}
# define a matrix A
A <- matrix(c(1, 1, 2, 1), nrow = 2)
A

# compute the eigenvalues of A and store in Lambda
eigen(A)
Lambda <- eigen(A)

# Print eigenvalues
Lambda$values[1]
Lambda$values[2]

# Verify that these numbers satisfy the conditions of being an eigenvalue
det(Lambda$values[1] * diag(2) - A)

det(Lambda$values[2] * diag(2) - A)

```

## 3.4 Computing Eigenvectors in R

 Find the eigenvectors of a matrix, and show that they satisfy the properties discussed in the lecture.
 
```{r}
# print eigenvectors
Lambda
print(Lambda$vectors[, 1])
print(Lambda$vectors[, 1])

# Verify that these eigenvectors & their associated eigenvalues satisfy Av - lambda V = 0

A %*% Lambda$vectors[, 1] - Lambda$values[1] * Lambda$vectors[, 1]
A %*% Lambda$vectors[, 2] - Lambda$values[2] * Lambda$vectors[, 2]
```
 

# 4.  Principal Component Analysis

"Big Data" is ubiquitous in data science and its applications. However, redundancy in these datasets can be problematic. In this chapter, we learn about principal component analysis and how it can be used in dimension reduction. 


One of the importance things that principal component analysis can do is shrink redundancy in your dataset. In its simplest manifestation, redundancy occurs when two variables are correlated. 

Principal Component Analysis: 

* One of the more-useful method from applied linear algebra

* Non-parametric way of extracting meaningful information from confusing data sets

* Uncover hidden, low-dimensional structures that underlie your data

* These structure are more-easily visualized and are often interpretable to content experts

**Theory**

$$\mathbf{A} = \left[\begin{array}
{rrr}
1 & 2 \\
2 & 4 \\
3 & 6 \\
4 & 8 \\
5 & 10 \\
\end{array}\right]
$$

The matrix $A^T$, the *transpose* of A, is the matrix made by interchanging the rows and columns of A. 

If your data set is in a matrix A, and the mean of each column has been subtracted for each element in a given column, then the *i*, $j^{th}$ element of the matrix

$$\frac{A^T*A}{n-1}$$

where *n* is the number of rows of A, is the *co-variance* between the variables in the $i^th$ and $j^th$ column of the data in the matrix. 

Hence, the $i^th$ element of the diagonal of $\frac{A^T*A}{n-1}$ is the *variance* of the ith column of the matrix. 

```{r}
# create a matrix A
A <- matrix(c(1, 2, 3, 4, 5, 2, 4, 6, 8, 10), ncol = 2)
A

# subtract the mean
A[ ,1] <- A[, 1] - mean(A[, 1])
A[, 2] <- A[ ,2] - mean(A[, 2])

A

# Calculate covariance
t(A) %*% A /(nrow(A) - 1)

cov(A[, 1] , A[, 2])
var(A[, 1])
var(A[, 2])

```


**PCA**

* The eigenvalues $\lambda_1, \lambda_2, ..., \lambda_n$ of $\frac{A^T*A}{n-1}$ are real, and their corresponding eigenvectors are *orthogonal*, or point in distinct directions. 

* The *total variance* of the data set is the sum of the eigenvalues of $\frac{A^T*A}{n-1}$. 

* These eigenvectors $v_1, v_2, ..., v_n$ are called the *principal components* of the data set in the matrix *A*. 

* The direction that $v_j$ points in can explain $\lambda_j$ of the total variance in the data set. If $\lambda_j$ or a subset of $\lambda_1, \lambda_2, ..., \lambda_n$ explain a significant amount of the total variance, there is an opportunity for dimension reduction. 

```{r}
# variance-covariance matrix 
var_covar_matrix <- t(A) %*% A/(nrow(A) - 1)
var_covar_matrix

# eigen values and vectors
eigen(var_covar_matrix)
```


## 4.1.1  PCA - NFL Player dataset  

```{r}
# Read the data set
combine <- read_csv("data/NFL_Player_dataset.csv")
head(combine)
```

### 4.1 Redundant Variables 
```{r}
head(select(combine, height:shuttle))

# plot: forty vs shuttle
ggplot(data = combine, aes(x = shuttle, y = forty)) + 
  geom_point(alpha = 0.7) + 
  geom_smooth(method = "lm")

# Find the correlation between variables forty and three_cone
cor(combine$forty, combine$three_cone)

# Find the correlation between variables vertical adn broad_jump
cor(combine$vertical, combine$broad_jump)

```

### 4.1.2 calculate variance-covarince matrix and its structure
```{r pca}
# Extract numerical elements 
A <- combine[, 5:12]

# Make A into a matrix
A <- as.matrix(A)

# subtract the mean of all columns

A <- apply(A, 2, function(x) x - mean(x))

colMeans(A) # all means = 0 now

# Variance-covariance calculations

# create matrix B of variance-covariance
B <- t(A) %*% A /(nrow(A) - 1)
B

# compare 1st element of B to 1st column of variance of A
B[1, 1]
var(A[,1])

# Compare 1st element of 2nd column and row element of B to 1st and 2nd columns of A 
B[1, 2]
B[2, 1]

var(A[, 1], A[, 2])
```

We need to understand the structure of the variance-covariance matrix of a dataset. 

### 4.1.3 Eigenanalyses of combine data

Evaluate the potential to reduce the dimension of the data

```{r}
# find eigenvalues of B
V <- eigen(B)

# Print eigenvalues
round(V$values, digits = 4)

# head of eigenvector
round(head(V$vectors), digits = 4)

# Estimate the variablity in the dataset can be explained by the first principal component
V$values[1]/sum(V$values) * 100
```

## 4.2 PCA is easy in R
### 4.2.1 Scaling Data Before PCA

When dealing with data that have features with different scales, it is often important to scale the data first. This is because data that larger values may sway the data even with relatively little variability. 

```{r}
# Scale columns 5-12 of combmine
B <- scale(combine[, 5:12])

# Print the first few rows of the data
head(B)

# Summarize the principle component analysis
pr_comp <- prcomp(B)
summary(prcomp(B))

# add pc1 and pc2 to data set

combine_with_pc1pc2 <- cbind(combine, pr_comp$x[, 1:2])

head(combine_with_pc1pc2)

# plot pc1 and pc2
ggplot(combine_with_pc1pc2, aes(x = PC1, y = PC2, color = position)) +
  geom_point(alpha = 0.6)
```

### 4.2.2 Summarizing PCA in R

As shown in the above figure, the categorical variable (position) seems to identify itself with clusters in the first two principle components. 

Perform the same analysis as in the previous exercise, but only use the subset of the data where position equals "WR". 

```{r}
# Subset combine only to "WR"
combine_WR <- subset(combine, position == "WR")

# scale columns 5-12 of combine_WR
B <- scale(combine_WR[, 5:12])

# Print the first few rows of the data
head(B)

# summarize the principal component analysis 
summary(prcomp(B))
```

It takes the first three principal components of the wide receiver subset of data to explain the same amount of variability as the first component in the larger dataset. Once a major variable is removed, there's a lot more structure to the data's principal components! 
