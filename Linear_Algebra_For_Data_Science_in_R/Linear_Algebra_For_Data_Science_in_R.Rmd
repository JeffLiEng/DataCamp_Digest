---
title: "Linear Algebra For Data Science in R"
author: "Jeff Li"
date: "`r Sys.Date()`"
output: 
  html_document: 
    toc: true
    toc_depth: 3
    toc_float: true
---


__Course Description__

Linear algebra is one of the most important set of tools in applied mathematics and data science. In this course, you’ll learn how to work with vectors and matrices, solve matrix-vector equations, perform eigenvalue/eigenvector analyses and use principal component analysis to do dimension reduction on real-world datasets. All analyses will be performed in R, one of the world’s most-popular programming languages.


Reference: Eager, E. Linear Algebra For Data Science in R, www.datacamp.com, Paid class: 10/18/2018 - 10/20/2018. 

Note: Some course materials have been modified to run locally and train students by Jeff Li. 


# (I) Load required libraries

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
```

# 1. Introduction to Linear Algebra

## 1.1 Create vector in R 
```{r}
# Creating three 3's and four 4's, respectively
rep(3, 3)
rep(4, 4)

# Creating a vector with the first three even numbers and the first three odd numbers
seq(2, 6, by = 2)
seq(1, 5, by = 2)

# Re-creating the previous four vectors using the 'c' command
c(3, 3, 3)
c(4, 4, 4, 4)

c(2, 4, 6)
c(1, 3, 5)
```

In **R**, We can create vectors using some simple commands. 


## 1.2 The Algebra of Vectors

```{r}
# create the vectors x, y, and z
(x <- 1:7)
(y <- seq(2, 14, by = 2))
(z <- c(1, 1, 2))

# Add x to y and print

print(x + y)

# Multiply z by 2 and print
print(2 * z)

# Multiply x and y by each other and print
print(x * y)

# Add x to z, if possible, and print
print(x + z)

```

## 1.3 Creating Matrices in R
```{r}
# Create a matrix of all 1's and all 2's that are 2 by 3 and 3 by 2, respectively
matrix(1, 2, 3)

print(matrix(2, 3, 2))

# Create a matrix and changing the byrow designation.
matrix(c(1, 2, 3, 2), nrow = 2, ncol = 2, byrow = FALSE)
matrix(c(1, 2, 3, 2), nrow = 2, ncol = 2, byrow = TRUE)

# Add A to the previously-created matrix
A <- matrix(1, 2, 2)
A + matrix(c(1, 2, 3, 2), nrow = 2, ncol = 2, byrow = TRUE)
```


## 1.4 Matrix Multiplication as a Transformation

* Stretch 

* Shrink

* Reflect 

The following examples show how multiplication by a matrix can alter a vector. 

```{r}
# (a) Stretch: Need to stretch the x (first) componenet of a vector by factor of 4: c(1, 1)
A <- matrix(c(4, 0, 0, 1), nrow = 2, ncol = 2)
A

A %*% c(1, 1)

# (b) Shrinks the y (second) componenet of a vector b <- c(1, 1) by 50%
B <- matrix(c(1, 0, 0, 0.5), nrow = 2, ncol =2 )

B %*% c(1, 1)

# (c) reflect a vector about the y-axis
A <- matrix(c(-1, 0, 0, 1), nrow = 2, ncol = 2)
A

A %*% c(2, 2)

# (d) reflect a vector about the x-axis
B <- matrix(c(1, 0, 0, -1), nrow = 2, ncol =2)
B %*% c(2, 2)

# (e) rotation

C <- matrix(c(-1, 0, 0, -1), nrow = 2, ncol =2 )
C %*% c(2, 2)

```

## 1.5 Matrix Multiplication 

* Matrix multiplication is a complex transformation. 

* Matrix multiplication - Order matters

* matrix inverse (x * 1/x = 1, A %*% Ainv = I)

```{r}
# Take the inverse of the 2 by 2 identity matrix
solve(diag(2))

# Take the inverse of the matrix A
A <- matrix(c(1, -1, 2, 2), nrow = 2, ncol = 2, byrow = FALSE)
A
Ainv <- solve(A)

# Multiply A by its inverse on the left
Ainv %*% A

# Multiply A by its inverse on the right
A %*% Ainv

```


# 2. Matrix-Vector Equations

Many machine learning algorithms boil down to solving a matrix-vector equation. In this chapter, you learn what matrix-vector equations are trying to accomplish and how to solve them in R.

## 2.1 The Meaning of Ax = b and Exploring WNBA data

A great deal of applied mathematics and statistics, as well as data science, ends in a matrix-vector equation of the form: 

$$A*\bar{X} = \bar{b}$$

In this chapter, we will work with a matrix-vector model for team strength in the Women's Nation Basketball Association (WNBA) at the conclusion of the 2017 season. These team strengths cab be used to predict who willwin a match between any two teams. 

The WNBA has 12 teams, so Messey Matrix M will be 12 x 12. 

```{r}
# Read the Massey Matrix M 
M <- read_csv("data/WNBA_Data_2017_M.csv")
rownames(M) <-  colnames(M)


# Read the vector of point differentials f 
f <- read_csv("data/WNBA_Data_2017_f.csv")
rownames(f) <- rownames(M)
f

# Find the sum of the first column
sum(M[, 1])

# Find the sum of the vector 
sum(f)

```

## 2.2 Matrix Inversibility and Adjusting the Messey Matrix 

If a matrix is not (computationally) invertible, then an adjustment needs to be made. 

M: already add a row of 1's on the bottom of the matrix M, column of -1's to the far right of M, and a 0 to the bottom of the vector of point differentials f .

## 2.3 2017 WNBA Ratings
```{r}
# Solve for r and rename column
M <- as.matrix(M)
f <- as.matrix(f)
r <- solve(M) %*% f

colnames(r) <- "Rating"

# print r
print(r)


# Find the rating vector using ginv
r <- MASS::ginv(M)%*%f
colnames(r) <- "Rating"
print(r)

```


# 3. Eigenvalues and Eigenvectors

Matrix operations are complex. Eigenvalue/eigenvector analyses allow you to decompose these operations into simpler ones for the sake of image recognition, genomic analysis, and more!

## 3.1 Scaling Different Axes

transform a two-dimensional vector so that the first element doubled in size, while the second element was cut by a third.

```{r}
# create the matxix A
A <- matrix(c(2, 0, 0, 0.66667), nrow = 2)
A

A %*% c(1, 1)
```
Multiplied by a matrix that contracts and stretches elements of a vector!


## 3.2 Find Eigenvalues in R

```{r}
# build A
A <- matrix(c(-1, 0, 0, 2, 7, 0, 4, 12, -4), nrow = 3)
A

# Show that 7 is an eigenvalue for A
x1 <- c(0.2425356, 0.9701425, 0)

A %*% x1 - 7 * x1

# Show that -4 is an eigenvalue for A
x2 <- c(-0.3789810, -0.6821657, 0.6253186)

A %*% x2 - (-4) * x2

# Show that -1 is an eigenvalue for A
x3 <- c(1, 0, 0)
A %*% x3 - (-1)*x3

# show that a scalar multipleof an eigenvector is still an eigenvector of a matrix
# Show that double an eigenvector is still an eigenvector
A%*%((2)*c(0.2425356, 0.9701425, 0)) - 7*(2)*c(0.2425356, 0.9701425, 0)

# Show half of an eigenvector is still an eigenvector
A%*%((0.5)*c(0.2425356, 0.9701425, 0)) - 7*(0.5)*c(0.2425356, 0.9701425, 0)
```

## 3.3 Verifying the Math on Eigenvalues

Find the eigenvalues of a matrix, and show that they satisfy the properties discussed in the lecture.

```{r}
# define a matrix A
A <- matrix(c(1, 1, 2, 1), nrow = 2)
A

# compute the eigenvalues of A and store in Lambda
eigen(A)
Lambda <- eigen(A)

# Print eigenvalues
Lambda$values[1]
Lambda$values[2]

# Verify that these numbers satisfy the conditions of being an eigenvalue
det(Lambda$values[1] * diag(2) - A)

det(Lambda$values[2] * diag(2) - A)

```

## 3.4 Computing Eigenvectors in R

 Find the eigenvectors of a matrix, and show that they satisfy the properties discussed in the lecture.
 
```{r}
# print eigenvectors
Lambda
print(Lambda$vectors[, 1])
print(Lambda$vectors[, 1])

# Verify that these eigenvectors & their associated eigenvalues satisfy Av - lambda V = 0

A %*% Lambda$vectors[, 1] - Lambda$values[1] * Lambda$vectors[, 1]
A %*% Lambda$vectors[, 2] - Lambda$values[2] * Lambda$vectors[, 2]
```
 

# 4.  Principal Component Analysis

"Big Data" is ubiquitous in data science and its applications. However, redundancy in these datasets can be problematic. In this chapter, we learn about pricipal component analysis and how it can be used in dimension reduction. 

