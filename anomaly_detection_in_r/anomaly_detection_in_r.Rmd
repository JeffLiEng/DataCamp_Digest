---
title: "Hyperparameter Tuning in R"
author: "Jeff Li"
date: "`r Sys.Date()`"
output: 
  html_document: 
    toc: true
    toc_depth: 3
    toc_float: true
---

"I maintained my edge by always being a student; you will always have something new to learn". - Jackie Joyner Kersee


**Course Description:**

For many machine learning problems, simply running a model out-of-the-box and getting a prediction is not enough; you want the best model with the most accurate prediction. One way to perfect your model is with hyperparameter tuning, which means optimizing the settings for that specific model. In this course, you will work with the caret, mlr and h2o packages to find the optimal combination of hyperparameters in an efficient manner using grid search, random search, adaptive resampling and automatic machine learning (AutoML). Furthermore, you will work with different datasets and tune different supervised learning models, such as random forests, gradient boosting machines, support vector machines, and even neural nets. Get ready to tune!

Ref: Glander, Shirin. "Hyperparameter Tuning in R", www.datacamp.com, 2019. 


## (I) Load Required Libraries
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
```

# 1. Introduction to hyperparameters 
Why do we use the strange word "hyperparameter"? What makes it hyper? Here, you will understand what model parameters are, and why they are different from hyperparameters in machine learning. You will then see why we would want to tune them and how the default setting of caret automatically includes hyperparameter tuning.


## 1.1 Parameters vs Hyperparameters

* Model **parameters** are being fit (i.e. found) during training, and they are the result of model fitting or training

* Model **hyperparameters** are being set before training, adn they specify **how** the training is supposed to happen

In the linear model, coefficients were found during fitting. **method** was an option ot set **before** fitting. 

In **machine learning**, weights and biases of neural nets that are optimized during training --> model parameters

Options like learning rate, weight decay adn number of trees in a Random Forest model that can be tweeked --> hyperparamters


```{r}
library(AnomalyDetection)

AnomalyDetectionTs(raw_data, max_anoms = 0.02, direction = "both", plot = TRUE)


```



