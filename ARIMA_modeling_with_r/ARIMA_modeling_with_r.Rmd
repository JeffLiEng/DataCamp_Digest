---
title: "ARIMA Modeling with R"
author: "Jeff Li"
date: "`r Sys.Date()`"
output: 
  html_document: 
    toc: true
    toc_depth: 3
    toc_float: true
---

"I maintained my edge by always being a student; you will always have something new to learn". - Jackie Joyner Kersee


**Course Description:**

In this course, you will become an expert in fitting ARIMA models to time series data using R. First, you will explore the nature of time series data using the tools in the R stats package. Next, you learn how to fit various ARMA models to simulated data (where you will know the correct model) using the R package astsa. Once you have mastered the basics, you will learn how to fit integrated ARMA models, or ARIMA models to various real data sets. You will learn how to check the validity of an ARIMA model and you will learn how to forecast time series data. Finally, you will learn how to fit ARIMA models to seasonal data, including forecasting using the astsa package.

Ref: Stoffer, David. (2019) "ARIMA Modeling with R", www.datacamp.com. 


## (I) Load Required Libraries
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)

library(astsa)

```


# 1. Time Series Data and Models 

"You will investigate the nature of time series data and learn the basics of ARMA models that can explain the behavior of such data. You will learn the basic R commands needed to help set up raw time series data to a form that can be analyzed using ARMA models."


## 1.1 Time series 

Time series are everywhere: finance, industrial processes, nature. 

* Auto-regressive (AR) & Moving Average (MA): ARMA

* Integrated ARMA: ARIMA

```{r}
# Time Series data - 1
head(jj)
str(jj)
plot(jj, main = "Johnson & Johnson Quarterly Earning per Share", type = "c")
text(jj, labels = 1:4, col = 1:4)

# Time series data - II
str(globtemp)
plot(globtemp, main = "Global Temperature Deviations", type = "o")

# Time series data - III
library(xts)
str(sp500w)
head(sp500w)
plot(sp500w, main = "S&P 500 Weekly Returns")
```


## 1.2 Time Series Regression Models

Regression: $Y_i = \beta*X_i + \epsilon_i$, where $\epsilon_i$ is white noise. 

White Noise: 

  * independent normal with common variance
  
  * is basic building blocks of time series
  
AutoRegression: $X_t = \phi * X_{t-1} + \epsilon_t$ ($\epsilon$ is white noise)

Moving Average: $\epsilon_t = W_t + \theta * W_{t-1}$ ($W_t$ is white noise)

ARMA: $X_t = \phi * X_{t-1} + W_t + \theta * W_{t-1}$ 


```{r}
# View a detailed description of AirPassengers
help(AirPassengers)

# Plot AirPassengers
plot(AirPassengers)

# Plot the DJIA daily closings
plot(djia$Close)

# Plot the Southern Oscillation Index
plot(soi)

```

The AirPassengers data show a handful of important qualities, including seasonality, trend, and heteroscedasticity, which distinguish the data from standard white noise. 



## 1.3 Stationarity and Nonstationarity 

Stationarity: The mean is constant over time (no trend); the correlation structure remains constant over time. 

Random Walk Trend: no stationary, but differenced data are stationary. 

Trend stationarity: sationarity around a trend, differencing still works!

Nonstationarity in trend and variability: First log, then difference. 


## 1.3.1 Differencing 

When a time series is trend stationary, it will have stationary behavior around a trend. A simple example is $Y_t = \alpha + \beta*t + X_t$ where $X_t$ is stationary. 

A different type of model for trend is *random walk*, which has the form $X_t = X_{t-1} + W_t$, where $W_t$ is white noise. It is called a random walk because at time t the process is where it was at time *t-1* plus a completely random movement. For a *random walk with drift*, a constant is added to the model and will cause the random walk to drift in the direction (positive or negative) of the drift. 

Differencing both trend stationary and random walk data has the effect of removing the trends, despite the important differences between the two datasets. 

```{r, eval=FALSE}
# Plot detrended y (trend stationary)
plot(diff(y))

# Plot detrended x (random walk)
plot(diff(x))

```


### 1.3.2 Detrending Data

Differencing is generally good for removing trend form time series data. 

```{r}
# Plot globtemp and detrended globtemp
par(mfrow = c(2, 1))
plot(globtemp)
plot(diff(globtemp))

# Plot cmort adn detrended cmort
par(mfrow = c(2, 1))
plot(cmort)
plot(diff(cmort))


```


## 1.3.3 Dealing with Trend and Heteroscedasticity 

We can coerce nonstationary data to stationarity by calculating the return or growth rate as follows. 

Often time series are generated as 
$$ X_i = (1 + p_i)X_{t-1}$$
meaning that the value of the time series observed at time t equals the value observed at time *t-1* and a small percent change *pi* at time t. 

A simple deterministic example is putting money into a bank with a fixed interest p. In the case, $X_i$ is the value of the account at time period *t* with an initial deposit of $X_0$. 

Typically, $p_t$ is referred to as the return or growth rate of a time series, and the process is often stable. 

The growth rate $p_i$ can be approximately by: 
$$Y_t = log(X_t) - log(X_{t-1}) $$ 

In R, $p_t$ is often calculated as *diff(log(x))* 

```{r}
# plot GNP series (gnp) and its growth rate
par(mfrow = c(2, 1)) 
plot(gnp)
plot(diff(log(gnp)))

# Plot DJIA closing (djia$Close) and its returns
par(mfrow = c(2, 1)) 
plot(djia$Close)
plot(diff(log(djia$Close)))
```


## 1.4 Stationary Time Series: ARMA

### 1.4.1 Wold Decomposition
Wold proved that any stationary time series may be represented as a linear combination of white noise: 
$$X_t = W_t + \alpha_1 W_{t-1} + \alpha_2 W_{t-2} + ... $$ 
For constants $\alpha_1, \alpha_2, ...$. 

Any ARMA model has this form, which means they are suited to modeling time series. 

### 1.4.2 Generating ARMA using arima.sim()

Basic syntax: *arima.sim(model, n, ...)* 

* *model* is a list with order of the model as c(p, d, q). p: order of AR, q: order of MA

Generate MA(1) given by: $X_t = W_t + 0.9*W_{t-1}$ 
```{r}
x <- arima.sim(list(order = c(0, 0, 1), ma = 0.9), n = 100)
plot(x)
```


Generating and plotting AR(x): $X_t = 0 * X_{t-1} -0.9*X_{t-2} + W_t$ 
```{r}
x <- arima.sim(list(order = c(2, 0, 0), ar = c(0, -0.9)), n = 100)
plot(x)
```

## 1.5 Simulating ARMA models

Any stationary time series can be written as a linear combination of white noise. 

R provides a simple function called *arima.sim()* to generate data from an ARMA model. 

```{r}
# Generate and plot white noise
WN <- arima.sim(model = list(order = c(0, 0, 0)), 
                n = 200)
plot(WN)

# Generate and plot an MA(1) with parameter .9
MA <- arima.sim(model = list(order = c(0, 0, 1), 
                             ma = 0.9), 
                n = 200)
plot(MA)

# Generate and plot an AR(2) with paraters 1.5 and -0.75
AR <- arima.sim(model = list(order = c(2, 0, 0), 
                             ar = c(1.5, -0.75)), 
                n = 200)
plot(AR)
```


# 2. Fitting ARMA models
Lean how to identify a model; how to choose the correct model; how to verify a model. 


## 2.1 AR and MA models

```{r}
# Generate AR and MA data
ar_x <- arima.sim(model = list(order = c(1, 0, 0), 
                               ar = -0.7), 
                  n = 200)

ma_y <- arima.sim(model = list(order = c(0, 0, 1), 
                               ma = -0.7), 
                  n = 200)

# plot AR and MA data
par(mfrow = c(1, 2))
plot(ar_x, main = "AR(1)")
plot(ma_y, main = "MR(1)")
```

## 2.2 ACF and PACF

* AR(p): ACF - tails off;      PACF - Cuts off lag p

* MA(q): ACF - cuts off lag q; PACF - Tails off

* ARMA(p, q): ACF- tails off; PACF - tails off; 

```{r}
# AR(1)
acf(ar_x)
pacf(ar_x)


# MA(1)
acf(ma_y)
pacf(ma_y)
```

## 2.3 Estimation

* Estimation for time series is similar to using least squares for regression

* Estimates are obtained numerically using ideas of Gauss and Newton. 

Example: 

**AR(2) with mean 50:
$$ X_t = 50 + 1.5(X_{t-1} - 50) - 0.75(X_{t-2} - 50) + W_t$$

```{r}
# generate simulated data
x <- arima.sim(model = list(order = c(2, 0, 0),
                            ar = c(1.5, -0.75)), 
               n = 200) + 50

# plot
plot(x)
acf(x)
pacf(x)

# fit 
x_fit <- sarima(x, p = 2, d = 0, q = 0)

x_fit$ttable
```

MA(1): 
$$X_t = W_t - 0.7 W_{t-1}$$

```{r}
# simulate MA(1) data
y <- arima.sim(model = list(order = c(0, 0, 1), 
                            ma = -0.7), 
               n = 200)

# plot, acf, pacf
plot(y)
acf(y)
pacf(y)

# fit using the package of "astsa"
y_fit <- sarima(y, p = 0, d = 0, q = 1)
y_fit$ttable
```

## 2.4 Fitting an AR(1) model

Simulated data from AR(1): 
$$X_t = 0.9 X_{t-1} + W_t$$

```{r}
# Generate 100 obervations from the AR(1) model
x <- arima.sim(model = list(order = c(1, 0, 0), 
                            ar = 0.9), 
               n = 100)


# plot the generated data
plot(x)

# plot the sample ACF and PACF pair
acf2(x)

# Fit an AR(1) to the data and examine the t-table
sarima(x, p = 1, d = 0, q = 0)

```

## 2.5 Fitting an AR(2) model

AR(2): 
$$X_t = 1.5X_{t-1} - 0.75X_{t-2} + W_t$$

```{r}
# Generate simulated data
x <- arima.sim(model = list(order = c(2, 0, 0), 
                            ar = c(1.5, -0.75)), 
               n = 200)

# Plot x
plot(x)

# plot the sample P/ACF of x
acf2(x)

# Fit an AR(2) to the data and example the t-table
sarima(x, p = 2, d = 0, q = 0)
```

## 2.6 Fitting an MA(1) Model

MA(1) model: 
$$X_t = W_t - 0.8W_{t-1}$$

```{r}
# Simulate MA(1) data 
x <- arima.sim(model = list(order = c(0, 0, 1), 
                            ma = -0.8), 
               n = 100)
# plot x
plot(x)

# Plot the sample P/ACF of x
acf2(x)

# Fit an MA(1) to the data and example the t-table
sarima(x, p = 0, d = 0, q = 1)
```


## 2.7 AR and MA together

Auto-regression with correlated errors:
$$X_t = \phi*X_{t-1} + W_t + \theta * W_{t-1}$$

$$X_t = 0.9X_{t-1} + W_t -0.4W_{t-1}$$

```{r}
# simulate ARMA(1, 1)
x <- arima.sim(model = list(order = c(1, 0, 1), 
                            ar = 0.9, 
                            ma = -0.4), 
               n = 200)

# Plot 
plot(x, main = "ARMA(1, 1)")

# PACF and ACF plot
acf2(x)

# estimation
x_fit <- sarima(x, p = 1, d = 0, q = 1)

#
x_fit$ttable
```

## 2.8 Fitting an ARMA model 

ARMA(2, 1) model:
$$X_t = X_{t-1} - 0.9X_{t-2} + W_t + 0.8W_{t-1}$$

```{r}
# simulate ARMA(2, 1) data
x <- arima.sim(model = list(order = c(2, 0, 1), 
                            ar = c(1, -0.9), 
                            ma = 0.8), 
               n = 250)

# Plot x
plot(x)

# Plot the sample P/ACF of x
acf2(x)

# Fit an ARMA(2, 1) to the data and examine the t-talbe
sarima(x, p = 2, d = 0, q = 1)
```


## 2.9 Model Choice and Residual Analysis

**AIC** and **BIC**: 

$$average(observed - predicted)^2 + k(p+q)$$ 

* AIC and BIC measure the error and penalize (differently) for adding parameters. 

* For example, AIC has k = 2 and BIC has k = log(n)

* Goal: find the model with the smallest AIC or BIC

```{r}
# Model choice: AR(1) vs MA(2) 
gnpgr <- diff(log(gnp))

sarima(gnpgr, p = 1, d = 0, q = 0)

sarima(gnpgr, p = 0, d = 0, q = 2)
```

**Bad Residuals**: 

* Pattern in the residuals

* ACF has large values

* Q-Q plot suggests normality

* Q-statistic - all points below line. 


## 2.10 Model Choice 
The best approach to fitting ARMA is to start with a low order model, and then try to add a parameter at a time to see if the results change. 

```{r}
# log and difference data
dl_varve <- diff(log(varve))
plot(dl_varve)
acf2(dl_varve)

# Fit an MA(1) to dl_varve
ma1 <- sarima(dl_varve, p = 0, d = 0, q = 1)
ma1$BIC

ma1

# Fit an MA(2) to dl_varve
ma2 <- sarima(dl_varve, p = 0, d = 0, q = 2)
ma2$BIC
# Fit an ARMA(1, 1) to dl_varve
arma11 <- sarima(dl_varve, p = 1, d = 0, q = 1)

c(ma1$BIC, ma1$BIC, arma11$BIC)

data.frame(model = c("MA(1)", "MA(2)", "ARMA(1,1)"), 
           AIC = c(ma1$AIC, ma2$AIC, arma11$AIC), 
           BIC = c(ma1$BIC, ma2$BIC, arma11$BIC))

```

AIC and BIC help to find the model with the smallest error using the least number of parameters. The idea is based on the parsimony principle, which is basic to all science and tells you to choose the simplest scientific explanation that fits the evidence. 

We should always examine the residuals because the model assumes the errors are Gaussian white noise. 


## 2.11 Crude oil data
```{r}
# oil data (in dollars per barrel)
plot(oil)

# Calculate approximate oil returns
oil_returns <- diff(log(oil))

# Plot oil_returns 
plot(oil_returns)

# Plot the P/ACF pair for oil_returns
acf2(oil_returns)

# fit a model
sarima(oil_returns, p = 1, d = 0, q = 1)
```


# 3. ARIMA Models

Now that you know how to fit ARMA models to stationary time series, you will learn about integrated ARMA (ARIMA) models for nonstationary time series. You will fit the models to real data using R time series commands from the stats and astsa packages.

## 3.1 Identifying ARIMA

A time series exhibits ARIMA behavior if the differenced data have ARMA behavior 
```{r}

# Simulation ARIMA(p = 1, d = 1, q = 0)
x <- arima.sim(model = list(order = c(1, 1, 0), 
                            ar = .9), 
               n = 200)

plot(x, main = "ARIMA(1, 1, 0)")

# plot diff(x)
diff_x <- diff(x)
plot(diff_x, main = "ARMA(p = 1, d = 0, q = 0")

# ACF and PCF
acf2(x)
acf2(diff(x))
```


## 3.2 ARIMA - Plug and Play

A time series is called ARIMA(p, d, q) if the differenced series (of order d) is ARMA(p, q). 

ARIMA model:
$$Y_t = 0.9 Y_{t-1} + W_t$$

where $Y_t = \Delta X_t = X_t - X_{t-1}$. In this case, the model is an ARIMA(1, 1, 0) because the differenced data are an autoregression of order one. 

```{r}
# simulated time series 
x <- arima.sim(model = list(order = c(1, 1, 0), 
                            ar = 0.9), 
               n = 200)

# plot x 
plot(x)

# plot the P/ACF pair of x
acf2(x)

# Plot the differenced data
plot(diff(x))

# Plot teh P/ACF pair of the differenced data
acf2(diff(x))
```


## 3.3 Simulated ARIMA

An ARIMA(2, 1, 0) model: 

$$Y_t = 1 + 1.5Y_{t-1} - 0.75Y_{t-2} + W_t$$ 
where $Y_t = \Delta X_t = X_t - X_{t-1}$. 

```{r}
# Simulate ARIMA(2, 1, 0) 
x <- arima.sim(model = list(order = c(2, 1, 0), 
                            ar = c(1.5, -0.75)), 
               n = 250) + 1

# Plot x 
plot(x)

plot(diff(x))

# Plot sample P/ACF of differenced data adn determine model
acf2(diff(x))

# Estimate parameters and examine output
sarima(x, p = 2, d = 1, q = 0)
```

## 3.4 Global Warming Data 

```{r}
# plot 
plot(globtemp)

# plot the sample P/ACF pair of the differenced data
acf2(diff(globtemp))

# Fit an ARIMA(1, 1, 1) model to globtemp
sarima(globtemp, p = 1, d = 1, q = 1)

# Fit an ARIMA(0, 1, 2) model to globtemp. 
sarima(globtemp, p = 0, d = 1, q = 2)

```


## 3.5 Diagnostics - Simulated Over-fitting 

Simulated data ARIMA(0, 1, 1): 

$$Y_t = W_t + 0.9W_{t-1}$$ 
where $Y_t = \Delta X_t = X_t - X_{t-1}$. 

```{r}
# simulate data
x <- arima.sim(model = list(order = c(0, 1, 1), 
                            ma = 0.9), 
               n = 250)

# Plot x
plot(x)
acf2(x)

# Plot sample P/ACF pair of the differenced data
acf2(diff(x))

# Fit the first model: ARIMA(0, 1, 1)
sarima(x, p = 0, d = 1, q = 1)

# Fit the second model ARIMA(0, 1, 2)
sarima(x, p = 0, d = 1, q = 2)

```
As you can see from the t-table, the second MA parameter is not significantly different from zero and the first MA parameter is approximately the same in each run. Also, the AIC and BIC both increase when the parameter is added. In addition, the residual analysis of your ARIMA(0,1,1) model is fine. All of these facts together indicate that you have a successful model fit.

## 3.6 Diagnostics - Global Temperatures

```{r}
# Fit ARIMA(0, 1, 2) to globtemp and check diagnostics
sarima(globtemp, p = 0, d = 1, q = 2)

# fit ARIMA(1, 1, 1) to globtemp and check diagnostics
sarima(globtemp, p = 1, d = 1, q = 1)
```


## 3.7 Forecasting Simulated ARIMA

```{r}
# Generate 120 observation from an ARIMA(1, 1, 0) model with AR parameter 0.9
x <- arima.sim(model = list(order = c(1, 1, 0), 
                            ar = 0.9), 
               n = 120)

# plot x
plot(x)
plot(diff(x))

# Plot P/ACF pair of differenced data
acf2(diff(x))

# Fit model 
sarima(x, p = 1, d = 1, q = 0)

# Forecast the data 20 time periods ahead
sarima.for(x, n.ahead = 20,  p = 1, d = 1, q = 0)

```

## 3.8 Forecasting Global Temperatures 

```{r}
# Fit an ARIMA(0, 1, 2) to globtemp and check the fit
sarima(globtemp, p = 0, d = 1, q = 2)

# Forecast data 35 years into the future
sarima.for(xdata = globtemp, n.ahead = 35, p = 0, d = 1, q = 2)
```


# 4. Seasonal ARIMA

## 4.1 Pure Seasonal Models

* Often collect data with a known seasonal component

* Air Passenger (1 cycle every S = 12 months)

* Johnson & Johnson Earnings (1 cycle every S = 4 quarters)

An $SAR(P=1)_{s=12}$: 

$$X_t = \Phi X_{t-12} + W_t$$


ACF and PACF of pure seasonal models: 

SAR(P)s: ACF --> tails off, PACF --> Cuts off lag PS
SMA(Q)s: ACF --> Cuts off lag QS, PACF --> tails off
SARMA(P, Q)s: ACF --> Tails off, PaCF --> Tails off 


## 4.2 Fit a Pure Seasonal Model

An example of the pure seasonal model: 

$$X_t = 0.9 X_{t-12} + W_t + 0.5W_{t-12}$$
which we would denote as **SARMA(p=1, q = 1)s=12** 

```{r, eval=FALSE}
# Plot sample P/ACF to lag 60 and compare to the true values
acf2(x, max.lag = 60)

# Fit the seasonal model to x
sarima(x, p = 0, d = 0, q = 0, P = 1, D = 0, Q = 1, S = 12)

```

## 4.3 Fit a Mixed Seasonal Model 

* Mixed model: $SARIMA(p, d, q) x (P, D, Q)_s$ model 

* SARIMA(0, 0, 1) x (1, 0, 0)12 model

$$X_t = \Phi X_{t-12} + W_t + \theta W_{t-1}$$

* SAR(1): Value this month is related to last year's value $X_{t-12}$ 

* MA(1): This month's value related to last month's shock $W_{t-1}$ 

SARIMA(0, 0, 1) x (1, 0, 0)s=12

$$X_t = 0.8X_{t-12} + W_t - 0.5 W_{t-1}$$ 

```{r, eval=FALSE}
# Plot sample P/ACF pair to lag 60 and compare to actual
acf2(x, max.lag = 60)

# Fit the seasonal model to x
sarima(x, p = 0, d = 0, q =1, P = 0, D = 0, Q = 1, S = 12 )

```


A simulated data: 
```{r}
# ARIMA(0,0,1) x (0, 0, 1)12
x <- arima.sim(model = list(order = c(0, 0, 12), 
                            ma = c(0.7, rep(0, 10), 0.9)), 
               n = 200)

# plot x
plot(x)

# P/ACF
acf2(x)
```


## 4.4 Data Analysis - Unemployment 
```{r}
# plot unemp: with trend adn seasonality 
plot(unemp)

# Difference the data and plot
d_unemp <- diff(unemp)
plot(d_unemp)

# Seasonally difference and plot it
dd_unemp <- diff(d_unemp, lag = 12)
plot(dd_unemp) # After removing the trend adn seasonal variation, the data appear to be stationary. 

# 
dd_umemp <- diff(diff(unemp), lag = 12)

# plot P/ACF
acf2(dd_umemp, max.lag = 60)

# Fit an appropriate model
sarima(unemp, 
       p = 2, d = 1, q = 0, 
       P = 0, D = 1, Q = 1, S = 12)

```


## 4.5 Data Analysis - Commodity Prices

Making money in commodities is not easy. Most commodities traders lose money rather than make it. 

"chicken" data: is the monthly whole bird spot price, Georgia docks, US cents per pound, from August 2001 to July 2016. 

After removing the trend, the sample ACF and PACF suggest an AR(2) model because the PACF cuts off after lag 2 and the ACF tails off. However, the ACF has a small seasonal component remaining. This can be taken care of by fitting an addition SAR(1) component.


```{r}
# plot differenced chicken
plot(chicken)
plot(diff(chicken))

# Plot P/ACF pair of differenced data to lag 60
acf2(diff(chicken), max.lag = 60)

# Fit ARIMA(2, 1, 0) to chicken - not so good
sarima(chicken, p = 2, d = 1, q = 0)

# Fit SARIMA(2, 1, 0, 1, 0, 0, 12) to chicken 

sarima(chicken, p = 2, d = 1, q = 0, P = 1, D = 0, Q = 0, S = 12)

```


## 4.6 Data Analysis - Birth Rate

*birth* time series: monthly live births (adjusted) in thousands for the United States, 1948-1979, and includes the baby boom after WWII. 

```{r}
# Plot 
plot(birth)

# difference the data 
d_birth <- diff(birth)
plot(d_birth)

# Plot P/ACF to lag 60 of differenced data
acf2(d_birth, max.lag = 60)

# Seasonal difference data
dd_birth <- diff(d_birth, lag = 12)
plot(dd_birth)
acf2(dd_birth, max.lag = 60)

# Fit SARIMA(0, 1, 1)x(0, 1, 1)_12.
sarima(birth, p = 0, d = 1, q = 1, P = 0, D = 1, Q = 1, S = 12)

# Add AR term  (include an additional nonseasonal AR team to account for the extra correlation)
sarima(birth, p = 1, d = 1, q = 1, P = 0, D = 1, Q = 1, S = 12)

```


## 4.7 Forecasting ARIMA Processes

Once model is chose, forecasting is easy because the model describes how the dynamics of the time series behave over time. 

Simply continue the model dynamics into the future. 

### 4.7.1 Forecasting Monthly Unemployment

```{r}
# Fit the previous model to unemp and check the diagnostics
sarima(unemp, 
       p = 2, d = 1, q = 0, 
       P = 0, D = 1, Q = 1, S = 12)

# Forecast the data 3 years into the future
sarima.for(xdata = unemp, n.ahead = 36, 
           p = 2, d = 1, q = 0, 
           P = 0, D = 1, Q = 1, S = 12)
```

### 4.7.2 How hard is it to forecast commodity prices? 

Making money in commodities is not easy. It is really hard to predict a commodity. 

This is because commodities are subject to many sources of variation. 

```{r}
# Fit the chicken again (SARIMA(2, 1, 0)(1, 0, 0)12 
sarima(chicken, 
       p = 2, d = 1, q = 0, 
       P = 1, D = 0, Q = 0, S = 12 )

# Use sarima.for() to forecast the data 5 years into the futre

sarima.for(chicken, n.ahead = 60, 
           p = 2, d = 1, q = 0, 
           P = 1, D = 0, Q = 0, S = 12)

```

